{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "a98b92d8",
        "execution_start": 1686919203555,
        "execution_millis": 13283,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "32d552da840d4b668d2312aa2b1310d6",
        "deepnote_cell_type": "code",
        "id": "zTBK_W8Jbe5R",
        "outputId": "11d01e90-fe51-4e00-e39a-94a84fa4f30c"
      },
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "import stanza\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import nltk\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/shared-libs/python3.9/py/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n2023-06-16 12:40:06.673618: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-06-16 12:40:06.865194: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-06-16 12:40:06.865223: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2023-06-16 12:40:06.900469: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-06-16 12:40:08.396829: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2023-06-16 12:40:08.396970: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2023-06-16 12:40:08.396986: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n2023-06-16 12:40:11.074737: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2023-06-16 12:40:11.074785: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n2023-06-16 12:40:11.074810: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (p-7159b134-6dce-4d72-befb-c34071b2c35b): /proc/driver/nvidia/version does not exist\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "5315f5b99ca9423f92a5e734761c5abf",
        "deepnote_cell_type": "text-cell-p",
        "id": "LBgr-j90be5X"
      },
      "source": [
        "We started by importing various libraries that provide functions and tools for text preprocessing, feature extraction, modeling, and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "fa891147",
        "execution_start": 1686919216848,
        "execution_millis": 197,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "275690582cf54fdc8eafe96090b925a9",
        "deepnote_cell_type": "code",
        "id": "t_ti_Cnbbe5Y"
      },
      "source": [
        "data = pd.read_csv('/work/-20230526-132402/yelp_review.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "731947e",
        "execution_start": 1686919217056,
        "execution_millis": 11,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "9e4d6a458bda4d208424d653677198db",
        "deepnote_cell_type": "code",
        "id": "MCPw3nuIbe5Z"
      },
      "source": [
        "data = data.dropna(subset=['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "517c473f",
        "execution_start": 1686919217075,
        "execution_millis": 6118,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "610ce8d23a0542aa86e7c17df349c3ca",
        "deepnote_cell_type": "code",
        "id": "ZABXuaWxbe5a",
        "outputId": "1296a833-b00d-4444-b78b-d8571b049cb3"
      },
      "source": [
        "spacy.cli.download(\"en_core_web_sm\")\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting en-core-web-sm==3.4.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.8/12.8 MB 64.3 MB/s eta 0:00:00\nRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from en-core-web-sm==3.4.1) (3.4.2)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.28.1)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\nRequirement already satisfied: pathy>=0.3.5 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.6.2)\nRequirement already satisfied: jinja2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\nRequirement already satisfied: packaging>=20.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\nRequirement already satisfied: wasabi<1.1.0,>=0.9.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\nRequirement already satisfied: typer<0.5.0,>=0.3.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.2)\nRequirement already satisfied: numpy>=1.15.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.23.4)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\nRequirement already satisfied: thinc<8.2.0,>=8.1.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\nRequirement already satisfied: setuptools in /root/venv/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (58.1.0)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\nRequirement already satisfied: smart-open<6.0.0,>=5.2.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\nRequirement already satisfied: typing-extensions>=4.1.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.4.0)\nRequirement already satisfied: charset-normalizer<3,>=2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.3)\nRequirement already satisfied: MarkupSafe>=0.23 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.0)\nInstalling collected packages: en-core-web-sm\nSuccessfully installed en-core-web-sm-3.4.1\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "9e2364747d7c4b15b03e466157704f63",
        "deepnote_cell_type": "text-cell-p",
        "id": "MxxdxHhlbe5a"
      },
      "source": [
        "Visualizing the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "74f553e",
        "execution_start": 1686919223201,
        "execution_millis": 19,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "fb1300ad46b8474d86064a96bdd6ba05",
        "deepnote_cell_type": "code",
        "id": "8UPY84gTbe5b"
      },
      "source": [
        "data=data[['stars','text']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "e6928a8f",
        "execution_start": 1686919223220,
        "execution_millis": 94,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "35136e4ab01c43de978119f04675eaef",
        "deepnote_cell_type": "code",
        "id": "PzrbJDqKbe5b",
        "outputId": "8685e168-221a-4e58-d765-057c78515ecb"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "application/vnd.deepnote.dataframe.v3+json": {
              "column_count": 2,
              "row_count": 10000,
              "columns": [
                {
                  "name": "stars",
                  "dtype": "int64",
                  "stats": {
                    "unique_count": 5,
                    "nan_count": 0,
                    "min": "1",
                    "max": "5",
                    "histogram": [
                      {
                        "bin_start": 1,
                        "bin_end": 1.4,
                        "count": 749
                      },
                      {
                        "bin_start": 1.4,
                        "bin_end": 1.8,
                        "count": 0
                      },
                      {
                        "bin_start": 1.8,
                        "bin_end": 2.2,
                        "count": 927
                      },
                      {
                        "bin_start": 2.2,
                        "bin_end": 2.6,
                        "count": 0
                      },
                      {
                        "bin_start": 2.6,
                        "bin_end": 3,
                        "count": 0
                      },
                      {
                        "bin_start": 3,
                        "bin_end": 3.4000000000000004,
                        "count": 1461
                      },
                      {
                        "bin_start": 3.4000000000000004,
                        "bin_end": 3.8000000000000003,
                        "count": 0
                      },
                      {
                        "bin_start": 3.8000000000000003,
                        "bin_end": 4.2,
                        "count": 3526
                      },
                      {
                        "bin_start": 4.2,
                        "bin_end": 4.6,
                        "count": 0
                      },
                      {
                        "bin_start": 4.6,
                        "bin_end": 5,
                        "count": 3337
                      }
                    ]
                  }
                },
                {
                  "name": "text",
                  "dtype": "object",
                  "stats": {
                    "unique_count": 9998,
                    "nan_count": 0,
                    "categories": [
                      {
                        "name": "Great service",
                        "count": 2
                      },
                      {
                        "name": "This review is for the chain in general. The location we went to is new so it isn't in Yelp yet. Once it is I will put this review there as well. We were there on Friday at 5 PM. \n\nThe reason I gave it 2 stars is because the burger was very good and it was made the way I asked for it. My husbands burger was not.\n\nBut, the server and the fries left a lot to be desired. Let me preface by saying that we had been to several other locations. I like my fries crispy. I ask for them well done, extra crispy, scorched, tortured hollow tubes. Whatever their buzz word is for well done. The location will comply. EVERY OTHER 5 GUYS HAS COMPLIED. But not the one at TATUM AND SHEA. She said that corporate said they are not to cook the fries that way. So if we were to put up with soggy fries - yes soggy, then we did not want them. \n\nShe also interrupted us several times which is rude. THEN she went and called corporate just to double check for us and she came to the table and said they said no they were not to cook them that way. Seriously? We did not ask for her to do this. She actually accused us of being undercover shoppers. We started to say something and then again- she interupted.\n\nListen, if you explain that our choice is not how the company wishes to present their product and we still choose to have them a different way, you should comply. It is after all our money and our decision. I was raised with the rules that #1 the customer is always right. And #2 if the customer is wrong REFER TO RULE NUMBER 1!!\n\nWe will not return. They have lost our business and I hope she loses her job.\nIf you want to try a really good burger AND FRIES place- go to Paradise Valley Burger Company at 40th Street and Bell. You will not be disappointed.",
                        "count": 2
                      },
                      {
                        "name": "9996 others",
                        "count": 9996
                      }
                    ]
                  }
                },
                {
                  "name": "_deepnote_index_column",
                  "dtype": "int64"
                }
              ],
              "rows": [
                {
                  "stars": 5,
                  "text": "My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\n\nDo yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\n\nWhile EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\n\nAnyway, I can't wait to go back!",
                  "_deepnote_index_column": 0
                },
                {
                  "stars": 5,
                  "text": "I have no idea why some people give bad reviews about this place. It goes to show you, you can please everyone. They are probably griping about something that their own fault...there are many people like that.\n\nIn any case, my friend and I arrived at about 5:50 PM this past Sunday. It was pretty crowded, more than I thought for a Sunday evening and thought we would have to wait forever to get a seat but they said we'll be seated when the girl comes back from seating someone else. We were seated at 5:52 and the waiter came and got our drink orders. Everyone was very pleasant from the host that seated us to the waiter to the server. The prices were very good as well. We placed our orders once we decided what we wanted at 6:02. We shared the baked spaghetti calzone and the small \"Here's The Beef\" pizza so we can both try them. The calzone was huge and we got the smallest one (personal) and got the small 11\" pizza. Both were awesome! My friend liked the pizza better and I liked the calzon…",
                  "_deepnote_index_column": 1
                },
                {
                  "stars": 4,
                  "text": "love the gyro plate. Rice is so good and I also dig their candy selection :)",
                  "_deepnote_index_column": 2
                },
                {
                  "stars": 5,
                  "text": "Rosie, Dakota, and I LOVE Chaparral Dog Park!!! It's very convenient and surrounded by a lot of paths, a desert xeriscape, baseball fields, ballparks, and a lake with ducks.\n\nThe Scottsdale Park and Rec Dept. does a wonderful job of keeping the park clean and shaded.  You can find trash cans and poopy-pick up mitts located all over the park and paths.\n\nThe fenced in area is huge to let the dogs run, play, and sniff!",
                  "_deepnote_index_column": 3
                },
                {
                  "stars": 5,
                  "text": "General Manager Scott Petello is a good egg!!! Not to go into detail, but let me assure you if you have any issues (albeit rare) speak with Scott and treat the guy with some respect as you state your case and I'd be surprised if you don't walk out totally satisfied as I just did. Like I always say..... \"Mistakes are inevitable, it's how we recover from them that is important\"!!!\n\nThanks to Scott and his awesome staff. You've got a customer for life!! .......... :^)",
                  "_deepnote_index_column": 4
                },
                {
                  "stars": 4,
                  "text": "Quiessence is, simply put, beautiful.  Full windows and earthy wooden walls give a feeling of warmth inside this restaurant perched in the middle of a farm.  The restaurant seemed fairly full even on a Tuesday evening; we had secured reservations just a couple days before.\n\nMy friend and I had sampled sandwiches at the Farm Kitchen earlier that week, and were impressed enough to want to eat at the restaurant.  The crisp, fresh veggies didn't disappoint: we ordered the salad with orange and grapefruit slices and the crudites to start.  Both were very good; I didn't even know how much I liked raw radishes and turnips until I tried them with their pesto and aioli sauces.\n\nFor entrees, I ordered the lamb and my friend ordered the pork shoulder.  Service started out very good, but trailed off quickly.  Waiting for our food took a very long time (a couple seated after us received and finished their entrees before we received our's), and no one bothered to explain the situation until the mai…",
                  "_deepnote_index_column": 5
                },
                {
                  "stars": 5,
                  "text": "Drop what you're doing and drive here. After I ate here I had to go back the next day for more.  The food is that good.\n\nThis cute little green building may have gone competely unoticed if I hadn't been driving down Palm Rd to avoid construction.  While waiting to turn onto 16th Street the \"Grand Opening\" sign caught my eye and my little yelping soul leaped for joy!  A new place to try!\n\nIt looked desolate from the outside but when I opened the door I was put at easy by the decor, smell and cleanliness inside.  I ordered dinner for two, to go.  The menu was awesome.  I loved seeing all the variety: poblano peppers, mole, mahi mahi, mushrooms...something wrapped in banana leaves.  It made it difficult to choose something.  Here's what I've had so far: La Condesa Shrimp Burro and Baja Sur Dogfish Shark Taco.  They are both were very delicious meals but the shrimp burro stole the show.  So much flavor.  I snagged some bites from my hubbys mole and mahi mahi burros- mmmm such a delight.  …",
                  "_deepnote_index_column": 6
                },
                {
                  "stars": 4,
                  "text": "Luckily, I didn't have to travel far to make my connecting flight. And for this, I thank you, Phoenix.\n\nMy brief layover was pleasant as the employees were kind and the flight was on time.  Hopefully, next time I can grace Phoenix with my presence for a little while longer.",
                  "_deepnote_index_column": 7
                },
                {
                  "stars": 4,
                  "text": "Definitely come for Happy hour! Prices are amazing, sake bombers for $3...Great atmosphere and wait staff was incredibly nice and right on to all of our needs, didn't have to ask for a thing They were always spot on...Place gets crowded in the evening especially if you plan on sitting outside. I only wish there were one in Apollo Beach or Brandon!",
                  "_deepnote_index_column": 8
                },
                {
                  "stars": 5,
                  "text": "Nobuo shows his unique talents with everything on the menu. Carefully crafted features with much to drink. Start with the pork belly buns and a stout. Then go on until you can no longer.",
                  "_deepnote_index_column": 9
                }
              ]
            },
            "text/plain": "      stars                                               text\n0         5  My wife took me here on my birthday for breakf...\n1         5  I have no idea why some people give bad review...\n2         4  love the gyro plate. Rice is so good and I als...\n3         5  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...\n4         5  General Manager Scott Petello is a good egg!!!...\n...     ...                                                ...\n9995      3  First visit...Had lunch here today - used my G...\n9996      4  Should be called house of deliciousness!\\n\\nI ...\n9997      4  I recently visited Olive and Ivy for business ...\n9998      2  My nephew just moved to Scottsdale recently so...\n9999      5  4-5 locations.. all 4.5 star average.. I think...\n\n[10000 rows x 2 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stars</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>My wife took me here on my birthday for breakf...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>I have no idea why some people give bad review...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>love the gyro plate. Rice is so good and I als...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>General Manager Scott Petello is a good egg!!!...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>3</td>\n      <td>First visit...Had lunch here today - used my G...</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>4</td>\n      <td>Should be called house of deliciousness!\\n\\nI ...</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>4</td>\n      <td>I recently visited Olive and Ivy for business ...</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>2</td>\n      <td>My nephew just moved to Scottsdale recently so...</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>5</td>\n      <td>4-5 locations.. all 4.5 star average.. I think...</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows × 2 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "7dba7bee8bbc458d8bc54d343b7410c4",
        "deepnote_cell_type": "text-cell-p",
        "id": "lnHQFH4dbe5c"
      },
      "source": [
        "The text column is considered as the main text data for analysis, and the stars column represents the target labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "68bc41dbec584e369a81332908a352ae",
        "deepnote_cell_type": "text-cell-h2",
        "id": "DAFqaZZlbe5c"
      },
      "source": [
        "## Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "3e90e799c83942df99b8422c47c71a10",
        "deepnote_cell_type": "text-cell-h3",
        "id": "-M7Sbgf7be5d"
      },
      "source": [
        "### Tokenization and lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "e5bb22fa",
        "execution_start": 1686919223323,
        "execution_millis": 4,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "093a7b3f364044d89947aef9064916b6",
        "deepnote_cell_type": "code",
        "id": "HE1YtIbRbe5d"
      },
      "source": [
        "reviews = data['text'].tolist() # we converted the 'text' column to a list of strings\n",
        "tokenized_reviews = []\n",
        "lemmatized_reviews = []\n",
        "batch_size = 1000 # we used a batch size because it was taking too long\n",
        "# so to improve the efficiency and speed of the processing\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "1ccd287",
        "execution_start": 1686919223385,
        "execution_millis": 253932,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "d4d18bd0d56f44cb8e2913f0284fd6f7",
        "deepnote_cell_type": "code",
        "id": "JnXAe2BObe5d"
      },
      "source": [
        "for i in range(0, len(reviews), batch_size):\n",
        "    batch_reviews = reviews[i:i+batch_size]\n",
        "    docs = list(nlp.pipe(batch_reviews))\n",
        "\n",
        "    # we extracted the tokens\n",
        "    batch_tokens = [[token.text for token in doc] for doc in docs]\n",
        "    tokenized_reviews.extend(batch_tokens)\n",
        "\n",
        "    # Doing Lemmatization\n",
        "    batch_lemmas = [[token.lemma_ for token in doc if token.lemma_ != '-PRON-'] for doc in docs]\n",
        "    lemmatized_reviews.extend(batch_lemmas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "a053902417ea4e88832dd09d167b3b5b",
        "deepnote_cell_type": "text-cell-p",
        "id": "DrozbvvFbe5e"
      },
      "source": [
        "After doing both, we add them to our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "a26819a8",
        "execution_start": 1686919477357,
        "execution_millis": 25,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "0b24b6be44d541dc89c87cb9e0b652eb",
        "deepnote_cell_type": "code",
        "id": "iI4NJWEMbe5e",
        "outputId": "43f846ec-59a2-4bac-89f5-c51a8670334b"
      },
      "source": [
        "data['tokenized_text'] = tokenized_reviews\n",
        "data['lemmatized_text'] = lemmatized_reviews\n",
        "\n",
        "# Print the first review and its tokenized version\n",
        "print('Original Review:')\n",
        "print(data['text'][0])\n",
        "\n",
        "print('Tokenized Review:')\n",
        "print(data['tokenized_text'][0])\n",
        "\n",
        "\n",
        "print('Lemmatized Review:')\n",
        "print(data['lemmatized_text'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Original Review:\nMy wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\n\nDo yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\n\nWhile EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\n\nAnyway, I can't wait to go back!\nTokenized Review:\n['My', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'was', 'excellent', '.', ' ', 'The', 'weather', 'was', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', 'grounds', 'an', 'absolute', 'pleasure', '.', ' ', 'Our', 'waitress', 'was', 'excellent', 'and', 'our', 'food', 'arrived', 'quickly', 'on', 'the', 'semi', '-', 'busy', 'Saturday', 'morning', '.', ' ', 'It', 'looked', 'like', 'the', 'place', 'fills', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', '.', '\\n\\n', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', '.', ' ', 'It', 'was', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', 'had', '.', ' ', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', '.', ' ', 'It', 'was', 'amazing', '.', '\\n\\n', 'While', 'EVERYTHING', 'on', 'the', 'menu', 'looks', 'excellent', ',', 'I', 'had', 'the', 'white', 'truffle', 'scrambled', 'eggs', 'vegetable', 'skillet', 'and', 'it', 'was', 'tasty', 'and', 'delicious', '.', ' ', 'It', 'came', 'with', '2', 'pieces', 'of', 'their', 'griddled', 'bread', 'with', 'was', 'amazing', 'and', 'it', 'absolutely', 'made', 'the', 'meal', 'complete', '.', ' ', 'It', 'was', 'the', 'best', '\"', 'toast', '\"', 'I', \"'ve\", 'ever', 'had', '.', '\\n\\n', 'Anyway', ',', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back', '!']\nLemmatized Review:\n['my', 'wife', 'take', 'I', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'be', 'excellent', '.', ' ', 'the', 'weather', 'be', 'perfect', 'which', 'make', 'sit', 'outside', 'overlook', 'their', 'ground', 'an', 'absolute', 'pleasure', '.', ' ', 'our', 'waitress', 'be', 'excellent', 'and', 'our', 'food', 'arrive', 'quickly', 'on', 'the', 'semi', '-', 'busy', 'Saturday', 'morning', '.', ' ', 'it', 'look', 'like', 'the', 'place', 'fill', 'up', 'pretty', 'quickly', 'so', 'the', 'early', 'you', 'get', 'here', 'the', 'well', '.', '\\n\\n', 'do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', '.', ' ', 'it', 'be', 'phenomenal', 'and', 'simply', 'the', 'good', 'I', \"'ve\", 'ever', 'have', '.', ' ', 'I', 'be', 'pretty', 'sure', 'they', 'only', 'use', 'ingredient', 'from', 'their', 'garden', 'and', 'blend', 'they', 'fresh', 'when', 'you', 'order', 'it', '.', ' ', 'it', 'be', 'amazing', '.', '\\n\\n', 'while', 'everything', 'on', 'the', 'menu', 'look', 'excellent', ',', 'I', 'have', 'the', 'white', 'truffle', 'scramble', 'egg', 'vegetable', 'skillet', 'and', 'it', 'be', 'tasty', 'and', 'delicious', '.', ' ', 'it', 'come', 'with', '2', 'piece', 'of', 'their', 'griddle', 'bread', 'with', 'be', 'amazing', 'and', 'it', 'absolutely', 'make', 'the', 'meal', 'complete', '.', ' ', 'it', 'be', 'the', 'good', '\"', 'toast', '\"', 'I', \"'ve\", 'ever', 'have', '.', '\\n\\n', 'anyway', ',', 'I', 'can', 'not', 'wait', 'to', 'go', 'back', '!']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "014dcd9fe0c84c819897b9e49d1edbe9",
        "deepnote_cell_type": "text-cell-p",
        "id": "Ca2gbBu7be5e"
      },
      "source": [
        "Adding additional pre-processing operations: these additional preprocessing steps contribute to cleaner, more meaningful, and consistent text data, which can lead to improved performance in sentiment analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "2f482041a6ea4b5f808dd86a7ff96b45",
        "deepnote_cell_type": "text-cell-h3",
        "id": "cXpSg0zMbe5f"
      },
      "source": [
        "### Handling punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "144e734b",
        "execution_start": 1686919477407,
        "execution_millis": 7760,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "392e9132e5e74379a9b330c4a8ffba06",
        "deepnote_cell_type": "code",
        "id": "lLHYN60ibe5f"
      },
      "source": [
        "import string\n",
        "def remove_punctuation(text):\n",
        "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "data['tokenized_text'] = data['tokenized_text'].apply(lambda x: [remove_punctuation(token) for token in x])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "fab80e4c",
        "execution_start": 1686919485220,
        "execution_millis": 23,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "26e3d92281a14172abca5454027aa420",
        "deepnote_cell_type": "code",
        "id": "A7dQ6TxGbe5f",
        "outputId": "497161bb-0df4-42f0-86ae-5f7f1fc6608a"
      },
      "source": [
        "print(data['tokenized_text'][0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "['My', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'was', 'excellent', '', ' ', 'The', 'weather', 'was', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', 'grounds', 'an', 'absolute', 'pleasure', '', ' ', 'Our', 'waitress', 'was', 'excellent', 'and', 'our', 'food', 'arrived', 'quickly', 'on', 'the', 'semi', '', 'busy', 'Saturday', 'morning', '', ' ', 'It', 'looked', 'like', 'the', 'place', 'fills', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', '', '\\n\\n', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', '', ' ', 'It', 'was', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', 've', 'ever', 'had', '', ' ', 'I', 'm', 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', '', ' ', 'It', 'was', 'amazing', '', '\\n\\n', 'While', 'EVERYTHING', 'on', 'the', 'menu', 'looks', 'excellent', '', 'I', 'had', 'the', 'white', 'truffle', 'scrambled', 'eggs', 'vegetable', 'skillet', 'and', 'it', 'was', 'tasty', 'and', 'delicious', '', ' ', 'It', 'came', 'with', '2', 'pieces', 'of', 'their', 'griddled', 'bread', 'with', 'was', 'amazing', 'and', 'it', 'absolutely', 'made', 'the', 'meal', 'complete', '', ' ', 'It', 'was', 'the', 'best', '', 'toast', '', 'I', 've', 'ever', 'had', '', '\\n\\n', 'Anyway', '', 'I', 'ca', 'nt', 'wait', 'to', 'go', 'back', '']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "063b8da9e204459f9512bbc6c45f5b4c",
        "deepnote_cell_type": "text-cell-h3",
        "id": "Jj74Asombe5f"
      },
      "source": [
        "### Removing stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "ccc6c012",
        "execution_start": 1686919485246,
        "execution_millis": 864,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "06b988b4bd214b689a65624e42618993",
        "deepnote_cell_type": "code",
        "id": "B1qm5uMRbe5g",
        "outputId": "45ac47c9-a68d-4ce4-b45b-0cfaee0b6794"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    return [token for token in tokens if token.lower() not in stopwords]\n",
        "data['tokenized_text'] = data['tokenized_text'].apply(remove_stopwords)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "dc375f97",
        "execution_start": 1686919486135,
        "execution_millis": 17,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "13acb159dfbb4632920d46295cbfb01a",
        "deepnote_cell_type": "code",
        "id": "ZQHJdxxKbe5g",
        "outputId": "a375032b-7a5c-40d1-8234-e04b1dbaed55"
      },
      "source": [
        "print(data['tokenized_text'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "['wife', 'took', 'birthday', 'breakfast', 'excellent', '', ' ', 'weather', 'perfect', 'made', 'sitting', 'outside', 'overlooking', 'grounds', 'absolute', 'pleasure', '', ' ', 'waitress', 'excellent', 'food', 'arrived', 'quickly', 'semi', '', 'busy', 'Saturday', 'morning', '', ' ', 'looked', 'like', 'place', 'fills', 'pretty', 'quickly', 'earlier', 'get', 'better', '', '\\n\\n', 'favor', 'get', 'Bloody', 'Mary', '', ' ', 'phenomenal', 'simply', 'best', 'ever', '', ' ', 'pretty', 'sure', 'use', 'ingredients', 'garden', 'blend', 'fresh', 'order', '', ' ', 'amazing', '', '\\n\\n', 'EVERYTHING', 'menu', 'looks', 'excellent', '', 'white', 'truffle', 'scrambled', 'eggs', 'vegetable', 'skillet', 'tasty', 'delicious', '', ' ', 'came', '2', 'pieces', 'griddled', 'bread', 'amazing', 'absolutely', 'made', 'meal', 'complete', '', ' ', 'best', '', 'toast', '', 'ever', '', '\\n\\n', 'Anyway', '', 'ca', 'nt', 'wait', 'go', 'back', '']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "d6058b6d678848dcb8e82fd3c2865423",
        "deepnote_cell_type": "text-cell-h3",
        "id": "PbPqegfSbe5g"
      },
      "source": [
        "### Lowercasing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "d75fd2ce",
        "execution_start": 1686919486169,
        "execution_millis": 585,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "1bd39be12f1746079e4e736e7ab5a74e",
        "deepnote_cell_type": "code",
        "id": "UoV4V01vbe5g"
      },
      "source": [
        "data['tokenized_text'] = data['tokenized_text'].apply(lambda x: [token.lower() for token in x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "dc375f97",
        "execution_start": 1686919486769,
        "execution_millis": 48,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "5ef420c54b9e4a58905ab5854bfacc79",
        "deepnote_cell_type": "code",
        "id": "bLxwWbZrbe5g",
        "outputId": "9a8c1195-42e5-49e5-9e44-490380c5118b"
      },
      "source": [
        "print(data['tokenized_text'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "['wife', 'took', 'birthday', 'breakfast', 'excellent', '', ' ', 'weather', 'perfect', 'made', 'sitting', 'outside', 'overlooking', 'grounds', 'absolute', 'pleasure', '', ' ', 'waitress', 'excellent', 'food', 'arrived', 'quickly', 'semi', '', 'busy', 'saturday', 'morning', '', ' ', 'looked', 'like', 'place', 'fills', 'pretty', 'quickly', 'earlier', 'get', 'better', '', '\\n\\n', 'favor', 'get', 'bloody', 'mary', '', ' ', 'phenomenal', 'simply', 'best', 'ever', '', ' ', 'pretty', 'sure', 'use', 'ingredients', 'garden', 'blend', 'fresh', 'order', '', ' ', 'amazing', '', '\\n\\n', 'everything', 'menu', 'looks', 'excellent', '', 'white', 'truffle', 'scrambled', 'eggs', 'vegetable', 'skillet', 'tasty', 'delicious', '', ' ', 'came', '2', 'pieces', 'griddled', 'bread', 'amazing', 'absolutely', 'made', 'meal', 'complete', '', ' ', 'best', '', 'toast', '', 'ever', '', '\\n\\n', 'anyway', '', 'ca', 'nt', 'wait', 'go', 'back', '']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "92add93da7d7468d9466189ef2a89394",
        "deepnote_cell_type": "text-cell-h3",
        "id": "DCSC7aYabe5h"
      },
      "source": [
        "### Handling negations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "1ec65c2d",
        "execution_start": 1686919486821,
        "execution_millis": 782,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "413e1b76cdd94b318d9bcb2c769cd77d",
        "deepnote_cell_type": "code",
        "id": "cqaj8gnLbe5h"
      },
      "source": [
        "def handle_negations(tokens):\n",
        "    negations = [\"not\", \"no\", \"n't\"]\n",
        "    handled_tokens = []\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        token = tokens[i]\n",
        "        if token in negations and i < len(tokens) - 1:\n",
        "            handled_tokens.append(token + \"_\" + tokens[i + 1])\n",
        "            i += 2\n",
        "        else:\n",
        "            handled_tokens.append(token)\n",
        "            i += 1\n",
        "    return handled_tokens\n",
        "\n",
        "data['tokenized_text'] = data['tokenized_text'].apply(handle_negations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "dc375f97",
        "execution_start": 1686919487618,
        "execution_millis": 10,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "f804349c65024bcc982741a0a4c649fd",
        "deepnote_cell_type": "code",
        "id": "axrQRne9be5h",
        "outputId": "e7916f84-269c-44d1-bc60-baede9ac3ba2"
      },
      "source": [
        "print(data['tokenized_text'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "['wife', 'took', 'birthday', 'breakfast', 'excellent', '', ' ', 'weather', 'perfect', 'made', 'sitting', 'outside', 'overlooking', 'grounds', 'absolute', 'pleasure', '', ' ', 'waitress', 'excellent', 'food', 'arrived', 'quickly', 'semi', '', 'busy', 'saturday', 'morning', '', ' ', 'looked', 'like', 'place', 'fills', 'pretty', 'quickly', 'earlier', 'get', 'better', '', '\\n\\n', 'favor', 'get', 'bloody', 'mary', '', ' ', 'phenomenal', 'simply', 'best', 'ever', '', ' ', 'pretty', 'sure', 'use', 'ingredients', 'garden', 'blend', 'fresh', 'order', '', ' ', 'amazing', '', '\\n\\n', 'everything', 'menu', 'looks', 'excellent', '', 'white', 'truffle', 'scrambled', 'eggs', 'vegetable', 'skillet', 'tasty', 'delicious', '', ' ', 'came', '2', 'pieces', 'griddled', 'bread', 'amazing', 'absolutely', 'made', 'meal', 'complete', '', ' ', 'best', '', 'toast', '', 'ever', '', '\\n\\n', 'anyway', '', 'ca', 'nt', 'wait', 'go', 'back', '']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "c8521299fb5847e9ad87d92d7d6c1368",
        "deepnote_cell_type": "text-cell-h3",
        "id": "kkIs4-TQbe5h"
      },
      "source": [
        "### Removing numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "8034bb07",
        "execution_start": 1686919487630,
        "execution_millis": 2268,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "748fcb02d2d44503ac9b8decd447b8e7",
        "deepnote_cell_type": "code",
        "id": "9nGA1z3hbe5h"
      },
      "source": [
        "def remove_numbers(tokens):\n",
        "    return [re.sub(r'\\d+', '', token) for token in tokens]\n",
        "\n",
        "data['tokenized_text'] = data['tokenized_text'].apply(remove_numbers)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "c5b916001bde460282459d0f02f17a60",
        "deepnote_cell_type": "text-cell-p",
        "id": "wNz2I9B-be5i"
      },
      "source": [
        "Now let's visualise the difference between our original review and the preprocessed review:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "8717ae78",
        "execution_start": 1686919489915,
        "execution_millis": 15,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "9d7dbd606adb41d59bf4e15ff80dea2a",
        "deepnote_cell_type": "code",
        "id": "N0aE0nLabe5i",
        "outputId": "ea13b728-0175-4009-eadf-e04ab455bbfc"
      },
      "source": [
        "print('Original Review:')\n",
        "print(data['text'][0])\n",
        "\n",
        "print('Preprocessed Review:')\n",
        "print(data['tokenized_text'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Original Review:\nMy wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\n\nDo yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\n\nWhile EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\n\nAnyway, I can't wait to go back!\nPreprocessed Review:\n['wife', 'took', 'birthday', 'breakfast', 'excellent', '', ' ', 'weather', 'perfect', 'made', 'sitting', 'outside', 'overlooking', 'grounds', 'absolute', 'pleasure', '', ' ', 'waitress', 'excellent', 'food', 'arrived', 'quickly', 'semi', '', 'busy', 'saturday', 'morning', '', ' ', 'looked', 'like', 'place', 'fills', 'pretty', 'quickly', 'earlier', 'get', 'better', '', '\\n\\n', 'favor', 'get', 'bloody', 'mary', '', ' ', 'phenomenal', 'simply', 'best', 'ever', '', ' ', 'pretty', 'sure', 'use', 'ingredients', 'garden', 'blend', 'fresh', 'order', '', ' ', 'amazing', '', '\\n\\n', 'everything', 'menu', 'looks', 'excellent', '', 'white', 'truffle', 'scrambled', 'eggs', 'vegetable', 'skillet', 'tasty', 'delicious', '', ' ', 'came', '', 'pieces', 'griddled', 'bread', 'amazing', 'absolutely', 'made', 'meal', 'complete', '', ' ', 'best', '', 'toast', '', 'ever', '', '\\n\\n', 'anyway', '', 'ca', 'nt', 'wait', 'go', 'back', '']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "945d1a2f91694b4f8df19545a3f379f1",
        "deepnote_cell_type": "text-cell-h2",
        "id": "YOI7K1Xibe5i"
      },
      "source": [
        "## Extracting linguistic features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "36cd4f14",
        "execution_start": 1686919489942,
        "execution_millis": 485,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "e2a70aa3e86c43cbb854447f41982214",
        "deepnote_cell_type": "code",
        "id": "0fyC_sOmbe5i"
      },
      "source": [
        "from collections import Counter\n",
        "def extract_linguistic_features(tokens):\n",
        "    features = Counter(tokens)\n",
        "    return features\n",
        "\n",
        "data['linguistic_features'] = data['tokenized_text'].apply(extract_linguistic_features)\n",
        "#counting the frequency of each unique token in the list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "4b4ff884",
        "execution_start": 1686919490455,
        "execution_millis": 12,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "c28ca58ae4bc4c5ea1dda1accb1a0e99",
        "deepnote_cell_type": "code",
        "id": "ap8WrfNEbe5i",
        "outputId": "332a4149-b081-4b38-b738-4c52fbb5b1e4"
      },
      "source": [
        "print('Original Review:')\n",
        "print(data['text'][0])\n",
        "\n",
        "print('Tokenized Review:')\n",
        "print(data['tokenized_text'][0])\n",
        "\n",
        "print('Linguistic Features:')\n",
        "print(data['linguistic_features'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Original Review:\nMy wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\n\nDo yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\n\nWhile EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\n\nAnyway, I can't wait to go back!\nTokenized Review:\n['wife', 'took', 'birthday', 'breakfast', 'excellent', '', ' ', 'weather', 'perfect', 'made', 'sitting', 'outside', 'overlooking', 'grounds', 'absolute', 'pleasure', '', ' ', 'waitress', 'excellent', 'food', 'arrived', 'quickly', 'semi', '', 'busy', 'saturday', 'morning', '', ' ', 'looked', 'like', 'place', 'fills', 'pretty', 'quickly', 'earlier', 'get', 'better', '', '\\n\\n', 'favor', 'get', 'bloody', 'mary', '', ' ', 'phenomenal', 'simply', 'best', 'ever', '', ' ', 'pretty', 'sure', 'use', 'ingredients', 'garden', 'blend', 'fresh', 'order', '', ' ', 'amazing', '', '\\n\\n', 'everything', 'menu', 'looks', 'excellent', '', 'white', 'truffle', 'scrambled', 'eggs', 'vegetable', 'skillet', 'tasty', 'delicious', '', ' ', 'came', '', 'pieces', 'griddled', 'bread', 'amazing', 'absolutely', 'made', 'meal', 'complete', '', ' ', 'best', '', 'toast', '', 'ever', '', '\\n\\n', 'anyway', '', 'ca', 'nt', 'wait', 'go', 'back', '']\nLinguistic Features:\nCounter({'': 18, ' ': 8, 'excellent': 3, '\\n\\n': 3, 'made': 2, 'quickly': 2, 'pretty': 2, 'get': 2, 'best': 2, 'ever': 2, 'amazing': 2, 'wife': 1, 'took': 1, 'birthday': 1, 'breakfast': 1, 'weather': 1, 'perfect': 1, 'sitting': 1, 'outside': 1, 'overlooking': 1, 'grounds': 1, 'absolute': 1, 'pleasure': 1, 'waitress': 1, 'food': 1, 'arrived': 1, 'semi': 1, 'busy': 1, 'saturday': 1, 'morning': 1, 'looked': 1, 'like': 1, 'place': 1, 'fills': 1, 'earlier': 1, 'better': 1, 'favor': 1, 'bloody': 1, 'mary': 1, 'phenomenal': 1, 'simply': 1, 'sure': 1, 'use': 1, 'ingredients': 1, 'garden': 1, 'blend': 1, 'fresh': 1, 'order': 1, 'everything': 1, 'menu': 1, 'looks': 1, 'white': 1, 'truffle': 1, 'scrambled': 1, 'eggs': 1, 'vegetable': 1, 'skillet': 1, 'tasty': 1, 'delicious': 1, 'came': 1, 'pieces': 1, 'griddled': 1, 'bread': 1, 'absolutely': 1, 'meal': 1, 'complete': 1, 'toast': 1, 'anyway': 1, 'ca': 1, 'nt': 1, 'wait': 1, 'go': 1, 'back': 1})\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "cc6043a1",
        "execution_start": 1686919490487,
        "execution_millis": 299580,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "1258fba0816d4839b7c4bbf4e7e27adb",
        "deepnote_cell_type": "code",
        "id": "oQTZXUJLbe5j"
      },
      "source": [
        "def extract_features(doc):\n",
        "    features = []\n",
        "    for token in doc:\n",
        "        features.append({\n",
        "            'token': token.text,\n",
        "            'lemma': token.lemma_,\n",
        "            'pos': token.pos_,\n",
        "            'tag': token.tag_,\n",
        "            'dep': token.dep_,\n",
        "            'is_stop': token.is_stop,\n",
        "            'is_alpha': token.is_alpha,\n",
        "        })\n",
        "    return features\n",
        "\n",
        "data['linguistic_features'] = data['tokenized_text'].apply(lambda x: extract_features(nlp(\" \".join(x))))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "7e365ace",
        "execution_start": 1686919790070,
        "execution_millis": 45,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "a88ec9e174ea4ba0a02096c9fb66639d",
        "deepnote_cell_type": "code",
        "id": "VNMFhuGnbe5j",
        "outputId": "5d79b742-7faa-4031-ed58-745ef01be302"
      },
      "source": [
        "print('Linguistic Features:')\n",
        "print(data['linguistic_features'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Linguistic Features:\n[{'token': 'wife', 'lemma': 'wife', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': 'took', 'lemma': 'take', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'ROOT', 'is_stop': False, 'is_alpha': True}, {'token': 'birthday', 'lemma': 'birthday', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'breakfast', 'lemma': 'breakfast', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'excellent', 'lemma': 'excellent', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': '   ', 'lemma': '   ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'weather', 'lemma': 'weather', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'perfect', 'lemma': 'perfect', 'pos': 'ADV', 'tag': 'RB', 'dep': 'dobj', 'is_stop': False, 'is_alpha': True}, {'token': 'made', 'lemma': 'make', 'pos': 'VERB', 'tag': 'VBN', 'dep': 'acl', 'is_stop': True, 'is_alpha': True}, {'token': 'sitting', 'lemma': 'sit', 'pos': 'VERB', 'tag': 'VBG', 'dep': 'xcomp', 'is_stop': False, 'is_alpha': True}, {'token': 'outside', 'lemma': 'outside', 'pos': 'ADP', 'tag': 'IN', 'dep': 'prep', 'is_stop': False, 'is_alpha': True}, {'token': 'overlooking', 'lemma': 'overlook', 'pos': 'VERB', 'tag': 'VBG', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': 'grounds', 'lemma': 'ground', 'pos': 'NOUN', 'tag': 'NNS', 'dep': 'pobj', 'is_stop': False, 'is_alpha': True}, {'token': 'absolute', 'lemma': 'absolute', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': 'pleasure', 'lemma': 'pleasure', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': '   ', 'lemma': '   ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'waitress', 'lemma': 'waitres', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'conj', 'is_stop': False, 'is_alpha': True}, {'token': 'excellent', 'lemma': 'excellent', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': 'food', 'lemma': 'food', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'dobj', 'is_stop': False, 'is_alpha': True}, {'token': 'arrived', 'lemma': 'arrive', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'conj', 'is_stop': False, 'is_alpha': True}, {'token': 'quickly', 'lemma': 'quickly', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': False, 'is_alpha': True}, {'token': 'semi', 'lemma': 'semi', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'busy', 'lemma': 'busy', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'advcl', 'is_stop': False, 'is_alpha': True}, {'token': 'saturday', 'lemma': 'saturday', 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'morning', 'lemma': 'morning', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'npadvmod', 'is_stop': False, 'is_alpha': True}, {'token': '   ', 'lemma': '   ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'looked', 'lemma': 'look', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'conj', 'is_stop': False, 'is_alpha': True}, {'token': 'like', 'lemma': 'like', 'pos': 'ADP', 'tag': 'IN', 'dep': 'prep', 'is_stop': False, 'is_alpha': True}, {'token': 'place', 'lemma': 'place', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'pobj', 'is_stop': False, 'is_alpha': True}, {'token': 'fills', 'lemma': 'fill', 'pos': 'VERB', 'tag': 'VBZ', 'dep': 'conj', 'is_stop': False, 'is_alpha': True}, {'token': 'pretty', 'lemma': 'pretty', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': False, 'is_alpha': True}, {'token': 'quickly', 'lemma': 'quickly', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': False, 'is_alpha': True}, {'token': 'earlier', 'lemma': 'early', 'pos': 'ADV', 'tag': 'RBR', 'dep': 'advmod', 'is_stop': False, 'is_alpha': True}, {'token': 'get', 'lemma': 'get', 'pos': 'VERB', 'tag': 'VBP', 'dep': 'conj', 'is_stop': True, 'is_alpha': True}, {'token': 'better', 'lemma': 'well', 'pos': 'ADJ', 'tag': 'JJR', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': ' \\n\\n ', 'lemma': ' \\n\\n ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'favor', 'lemma': 'favor', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'dobj', 'is_stop': False, 'is_alpha': True}, {'token': 'get', 'lemma': 'get', 'pos': 'VERB', 'tag': 'VBP', 'dep': 'conj', 'is_stop': True, 'is_alpha': True}, {'token': 'bloody', 'lemma': 'bloody', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': 'mary', 'lemma': 'mary', 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': '   ', 'lemma': '   ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'phenomenal', 'lemma': 'phenomenal', 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'dobj', 'is_stop': False, 'is_alpha': True}, {'token': 'simply', 'lemma': 'simply', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': False, 'is_alpha': True}, {'token': 'best', 'lemma': 'well', 'pos': 'ADV', 'tag': 'RBS', 'dep': 'advmod', 'is_stop': False, 'is_alpha': True}, {'token': 'ever', 'lemma': 'ever', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': True, 'is_alpha': True}, {'token': '   ', 'lemma': '   ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'pretty', 'lemma': 'pretty', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': False, 'is_alpha': True}, {'token': 'sure', 'lemma': 'sure', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': False, 'is_alpha': True}, {'token': 'use', 'lemma': 'use', 'pos': 'VERB', 'tag': 'VBP', 'dep': 'conj', 'is_stop': False, 'is_alpha': True}, {'token': 'ingredients', 'lemma': 'ingredient', 'pos': 'NOUN', 'tag': 'NNS', 'dep': 'dobj', 'is_stop': False, 'is_alpha': True}, {'token': 'garden', 'lemma': 'garden', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'dobj', 'is_stop': False, 'is_alpha': True}, {'token': 'blend', 'lemma': 'blend', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nmod', 'is_stop': False, 'is_alpha': True}, {'token': 'fresh', 'lemma': 'fresh', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': 'order', 'lemma': 'order', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'dobj', 'is_stop': False, 'is_alpha': True}, {'token': '   ', 'lemma': '   ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'amazing', 'lemma': 'amazing', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': ' \\n\\n ', 'lemma': ' \\n\\n ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'everything', 'lemma': 'everything', 'pos': 'PRON', 'tag': 'NN', 'dep': 'compound', 'is_stop': True, 'is_alpha': True}, {'token': 'menu', 'lemma': 'menu', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': 'looks', 'lemma': 'look', 'pos': 'VERB', 'tag': 'VBZ', 'dep': 'conj', 'is_stop': False, 'is_alpha': True}, {'token': 'excellent', 'lemma': 'excellent', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'acomp', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'white', 'lemma': 'white', 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': 'truffle', 'lemma': 'truffle', 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': 'scrambled', 'lemma': 'scramble', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': 'eggs', 'lemma': 'eggs', 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'vegetable', 'lemma': 'vegetable', 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'skillet', 'lemma': 'skillet', 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'dobj', 'is_stop': False, 'is_alpha': True}, {'token': 'tasty', 'lemma': 'tasty', 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'dobj', 'is_stop': False, 'is_alpha': True}, {'token': 'delicious', 'lemma': 'delicious', 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': '   ', 'lemma': '   ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'came', 'lemma': 'come', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'conj', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'pieces', 'lemma': 'piece', 'pos': 'NOUN', 'tag': 'NNS', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': 'griddled', 'lemma': 'griddle', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'ccomp', 'is_stop': False, 'is_alpha': True}, {'token': 'bread', 'lemma': 'bread', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'dobj', 'is_stop': False, 'is_alpha': True}, {'token': 'amazing', 'lemma': 'amazing', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'advcl', 'is_stop': False, 'is_alpha': True}, {'token': 'absolutely', 'lemma': 'absolutely', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': False, 'is_alpha': True}, {'token': 'made', 'lemma': 'make', 'pos': 'VERB', 'tag': 'VBN', 'dep': 'amod', 'is_stop': True, 'is_alpha': True}, {'token': 'meal', 'lemma': 'meal', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': 'complete', 'lemma': 'complete', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': '   ', 'lemma': '   ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'best', 'lemma': 'good', 'pos': 'ADJ', 'tag': 'JJS', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'toast', 'lemma': 'toast', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'npadvmod', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'ever', 'lemma': 'ever', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': True, 'is_alpha': True}, {'token': ' \\n\\n ', 'lemma': ' \\n\\n ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'anyway', 'lemma': 'anyway', 'pos': 'INTJ', 'tag': 'UH', 'dep': 'intj', 'is_stop': True, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'ca', 'lemma': 'can', 'pos': 'AUX', 'tag': 'MD', 'dep': 'aux', 'is_stop': True, 'is_alpha': True}, {'token': 'nt', 'lemma': 'not', 'pos': 'PART', 'tag': 'RB', 'dep': 'advmod', 'is_stop': False, 'is_alpha': True}, {'token': 'wait', 'lemma': 'wait', 'pos': 'VERB', 'tag': 'VB', 'dep': 'ROOT', 'is_stop': False, 'is_alpha': True}, {'token': 'go', 'lemma': 'go', 'pos': 'VERB', 'tag': 'VB', 'dep': 'xcomp', 'is_stop': True, 'is_alpha': True}, {'token': 'back', 'lemma': 'back', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': True, 'is_alpha': True}]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "e6928a8f",
        "execution_start": 1686919790114,
        "execution_millis": 4515,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "b16e1c2ff8054628a838231cf0ee1bf7",
        "deepnote_cell_type": "code",
        "id": "sKkmnsvJbe5j",
        "outputId": "c4adbcd8-b4d9-439a-cffa-674f5590cafb"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 24,
          "data": {
            "application/vnd.deepnote.dataframe.v3+json": {
              "column_count": 5,
              "row_count": 10000,
              "columns": [
                {
                  "name": "stars",
                  "dtype": "int64",
                  "stats": {
                    "unique_count": 5,
                    "nan_count": 0,
                    "min": "1",
                    "max": "5",
                    "histogram": [
                      {
                        "bin_start": 1,
                        "bin_end": 1.4,
                        "count": 749
                      },
                      {
                        "bin_start": 1.4,
                        "bin_end": 1.8,
                        "count": 0
                      },
                      {
                        "bin_start": 1.8,
                        "bin_end": 2.2,
                        "count": 927
                      },
                      {
                        "bin_start": 2.2,
                        "bin_end": 2.6,
                        "count": 0
                      },
                      {
                        "bin_start": 2.6,
                        "bin_end": 3,
                        "count": 0
                      },
                      {
                        "bin_start": 3,
                        "bin_end": 3.4000000000000004,
                        "count": 1461
                      },
                      {
                        "bin_start": 3.4000000000000004,
                        "bin_end": 3.8000000000000003,
                        "count": 0
                      },
                      {
                        "bin_start": 3.8000000000000003,
                        "bin_end": 4.2,
                        "count": 3526
                      },
                      {
                        "bin_start": 4.2,
                        "bin_end": 4.6,
                        "count": 0
                      },
                      {
                        "bin_start": 4.6,
                        "bin_end": 5,
                        "count": 3337
                      }
                    ]
                  }
                },
                {
                  "name": "text",
                  "dtype": "object",
                  "stats": {
                    "unique_count": 9998,
                    "nan_count": 0,
                    "categories": [
                      {
                        "name": "Great service",
                        "count": 2
                      },
                      {
                        "name": "This review is for the chain in general. The location we went to is new so it isn't in Yelp yet. Once it is I will put this review there as well. We were there on Friday at 5 PM. \n\nThe reason I gave it 2 stars is because the burger was very good and it was made the way I asked for it. My husbands burger was not.\n\nBut, the server and the fries left a lot to be desired. Let me preface by saying that we had been to several other locations. I like my fries crispy. I ask for them well done, extra crispy, scorched, tortured hollow tubes. Whatever their buzz word is for well done. The location will comply. EVERY OTHER 5 GUYS HAS COMPLIED. But not the one at TATUM AND SHEA. She said that corporate said they are not to cook the fries that way. So if we were to put up with soggy fries - yes soggy, then we did not want them. \n\nShe also interrupted us several times which is rude. THEN she went and called corporate just to double check for us and she came to the table and said they said no they were not to cook them that way. Seriously? We did not ask for her to do this. She actually accused us of being undercover shoppers. We started to say something and then again- she interupted.\n\nListen, if you explain that our choice is not how the company wishes to present their product and we still choose to have them a different way, you should comply. It is after all our money and our decision. I was raised with the rules that #1 the customer is always right. And #2 if the customer is wrong REFER TO RULE NUMBER 1!!\n\nWe will not return. They have lost our business and I hope she loses her job.\nIf you want to try a really good burger AND FRIES place- go to Paradise Valley Burger Company at 40th Street and Bell. You will not be disappointed.",
                        "count": 2
                      },
                      {
                        "name": "9996 others",
                        "count": 9996
                      }
                    ]
                  }
                },
                {
                  "name": "tokenized_text",
                  "dtype": "object",
                  "stats": {
                    "unique_count": 10000,
                    "nan_count": 0,
                    "categories": [
                      {
                        "name": "['great', 'service']",
                        "count": 2
                      },
                      {
                        "name": "['review', 'chain', 'general', '', 'location', 'went', 'new', 'nt', 'yelp', 'yet', '', 'put', 'review', 'well', '', 'friday', '', 'pm', '', '\\n\\n', 'reason', 'gave', '', 'stars', 'burger', 'good', 'made', 'way', 'asked', '', 'husbands', 'burger', '', '\\n\\n', '', 'server', 'fries', 'left', 'lot', 'desired', '', 'let', 'preface', 'saying', 'several', 'locations', '', 'like', 'fries', 'crispy', '', 'ask', 'well', 'done', '', 'extra', 'crispy', '', 'scorched', '', 'tortured', 'hollow', 'tubes', '', 'whatever', 'buzz', 'word', 'well', 'done', '', 'location', 'comply', '', 'every', '', 'guys', 'complied', '', 'one', 'tatum', 'shea', '', 'said', 'corporate', 'said', 'cook', 'fries', 'way', '', 'put', 'soggy', 'fries', '', 'yes', 'soggy', '', 'want', '', '\\n\\n', 'also', 'interrupted', 'us', 'several', 'times', 'rude', '', 'went', 'called', 'corporate', 'double', 'check', 'us', 'came', 'table', 'said', 'said', 'cook', 'way', '', 'seriously', '', 'ask', '', 'actually', 'accused', 'us', 'undercover', 'shoppers', '', 'started', 'say', 'something', 'interupted', '', '\\n\\n', 'listen', '', 'explain', 'choice', 'company', 'wishes', 'present', 'product', 'still', 'choose', 'different', 'way', '', 'comply', '', 'money', 'decision', '', 'raised', 'rules', '', '', 'customer', 'always', 'right', '', '', '', 'customer', 'wrong', 'refer', 'rule', 'number', '', '', '', '\\n\\n', 'return', '', 'lost', 'business', 'hope', 'loses', 'job', '', '\\n', 'want', 'try', 'really', 'good', 'burger', 'fries', 'place', 'go', 'paradise', 'valley', 'burger', 'company', 'th', 'street', 'bell', '', 'disappointed', '']",
                        "count": 2
                      },
                      {
                        "name": "9996 others",
                        "count": 9996
                      }
                    ]
                  }
                },
                {
                  "name": "lemmatized_text",
                  "dtype": "object",
                  "stats": {
                    "unique_count": 10000,
                    "nan_count": 0,
                    "categories": [
                      {
                        "name": "['great', 'service']",
                        "count": 2
                      },
                      {
                        "name": "['this', 'review', 'be', 'for', 'the', 'chain', 'in', 'general', '.', 'the', 'location', 'we', 'go', 'to', 'be', 'new', 'so', 'it', 'be', 'not', 'in', 'Yelp', 'yet', '.', 'once', 'it', 'be', 'I', 'will', 'put', 'this', 'review', 'there', 'as', 'well', '.', 'we', 'be', 'there', 'on', 'Friday', 'at', '5', 'pm', '.', '\\n\\n', 'the', 'reason', 'I', 'give', 'it', '2', 'star', 'be', 'because', 'the', 'burger', 'be', 'very', 'good', 'and', 'it', 'be', 'make', 'the', 'way', 'I', 'ask', 'for', 'it', '.', 'my', 'husband', 'burger', 'be', 'not', '.', '\\n\\n', 'but', ',', 'the', 'server', 'and', 'the', 'fry', 'leave', 'a', 'lot', 'to', 'be', 'desire', '.', 'let', 'I', 'preface', 'by', 'say', 'that', 'we', 'have', 'be', 'to', 'several', 'other', 'location', '.', 'I', 'like', 'my', 'fry', 'crispy', '.', 'I', 'ask', 'for', 'they', 'well', 'do', ',', 'extra', 'crispy', ',', 'scorch', ',', 'torture', 'hollow', 'tube', '.', 'whatever', 'their', 'buzz', 'word', 'be', 'for', 'well', 'do', '.', 'the', 'location', 'will', 'comply', '.', 'every', 'other', '5', 'GUYS', 'have', 'comply', '.', 'but', 'not', 'the', 'one', 'at', 'TATUM', 'and', 'SHEA', '.', 'she', 'say', 'that', 'corporate', 'say', 'they', 'be', 'not', 'to', 'cook', 'the', 'fry', 'that', 'way', '.', 'so', 'if', 'we', 'be', 'to', 'put', 'up', 'with', 'soggy', 'fry', '-', 'yes', 'soggy', ',', 'then', 'we', 'do', 'not', 'want', 'they', '.', '\\n\\n', 'she', 'also', 'interrupt', 'we', 'several', 'time', 'which', 'be', 'rude', '.', 'then', 'she', 'go', 'and', 'call', 'corporate', 'just', 'to', 'double', 'check', 'for', 'we', 'and', 'she', 'come', 'to', 'the', 'table', 'and', 'say', 'they', 'say', 'no', 'they', 'be', 'not', 'to', 'cook', 'they', 'that', 'way', '.', 'seriously', '?', 'we', 'do', 'not', 'ask', 'for', 'she', 'to', 'do', 'this', '.', 'she', 'actually', 'accuse', 'we', 'of', 'be', 'undercover', 'shopper', '.', 'we', 'start', 'to', 'say', 'something', 'and', 'then', 'again-', 'she', 'interupte', '.', '\\n\\n', 'listen', ',', 'if', 'you', 'explain', 'that', 'our', 'choice', 'be', 'not', 'how', 'the', 'company', 'wish', 'to', 'present', 'their', 'product', 'and', 'we', 'still', 'choose', 'to', 'have', 'they', 'a', 'different', 'way', ',', 'you', 'should', 'comply', '.', 'it', 'be', 'after', 'all', 'our', 'money', 'and', 'our', 'decision', '.', 'I', 'be', 'raise', 'with', 'the', 'rule', 'that', '#', '1', 'the', 'customer', 'be', 'always', 'right', '.', 'and', '#', '2', 'if', 'the', 'customer', 'be', 'wrong', 'refer', 'to', 'rule', 'number', '1', '!', '!', '\\n\\n', 'we', 'will', 'not', 'return', '.', 'they', 'have', 'lose', 'our', 'business', 'and', 'I', 'hope', 'she', 'lose', 'her', 'job', '.', '\\n', 'if', 'you', 'want', 'to', 'try', 'a', 'really', 'good', 'burger', 'and', 'FRIES', 'place-', 'go', 'to', 'Paradise', 'Valley', 'Burger', 'Company', 'at', '40th', 'Street', 'and', 'Bell', '.', 'you', 'will', 'not', 'be', 'disappoint', '.']",
                        "count": 2
                      },
                      {
                        "name": "9996 others",
                        "count": 9996
                      }
                    ]
                  }
                },
                {
                  "name": "linguistic_features",
                  "dtype": "object",
                  "stats": {
                    "unique_count": 10000,
                    "nan_count": 0,
                    "categories": [
                      {
                        "name": "[{'token': 'great', 'lemma': 'great', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': 'service', 'lemma': 'service', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'ROOT', 'is_stop': False, 'is_alpha': True}]",
                        "count": 2
                      },
                      {
                        "name": "[{'token': 'review', 'lemma': 'review', 'pos': 'VERB', 'tag': 'VB', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'chain', 'lemma': 'chain', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nmod', 'is_stop': False, 'is_alpha': True}, {'token': 'general', 'lemma': 'general', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'location', 'lemma': 'location', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': 'went', 'lemma': 'go', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'ROOT', 'is_stop': False, 'is_alpha': True}, {'token': 'new', 'lemma': 'new', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'acomp', 'is_stop': False, 'is_alpha': True}, {'token': 'nt', 'lemma': 'not', 'pos': 'PART', 'tag': 'RB', 'dep': 'advmod', 'is_stop': False, 'is_alpha': True}, {'token': 'yelp', 'lemma': 'yelp', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'acomp', 'is_stop': False, 'is_alpha': True}, {'token': 'yet', 'lemma': 'yet', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': True, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'put', 'lemma': 'put', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'conj', 'is_stop': True, 'is_alpha': True}, {'token': 'review', 'lemma': 'review', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'dobj', 'is_stop': False, 'is_alpha': True}, {'token': 'well', 'lemma': 'well', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': True, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'friday', 'lemma': 'friday', 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'npadvmod', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'pm', 'lemma': 'pm', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': ' \\n\\n ', 'lemma': ' \\n\\n ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'reason', 'lemma': 'reason', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': 'gave', 'lemma': 'give', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'conj', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'stars', 'lemma': 'star', 'pos': 'NOUN', 'tag': 'NNS', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'burger', 'lemma': 'burger', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'good', 'lemma': 'good', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'dobj', 'is_stop': False, 'is_alpha': True}, {'token': 'made', 'lemma': 'make', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'dep', 'is_stop': True, 'is_alpha': True}, {'token': 'way', 'lemma': 'way', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': 'asked', 'lemma': 'ask', 'pos': 'VERB', 'tag': 'VBN', 'dep': 'ccomp', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'husbands', 'lemma': 'husband', 'pos': 'NOUN', 'tag': 'NNS', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'burger', 'lemma': 'burger', 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': ' \\n\\n  ', 'lemma': ' \\n\\n  ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'server', 'lemma': 'server', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'fries', 'lemma': 'fry', 'pos': 'NOUN', 'tag': 'NNS', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': 'left', 'lemma': 'leave', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'ccomp', 'is_stop': False, 'is_alpha': True}, {'token': 'lot', 'lemma': 'lot', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'dobj', 'is_stop': False, 'is_alpha': True}, {'token': 'desired', 'lemma': 'desire', 'pos': 'VERB', 'tag': 'VBN', 'dep': 'acl', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'let', 'lemma': 'let', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'conj', 'is_stop': False, 'is_alpha': True}, {'token': 'preface', 'lemma': 'preface', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': 'saying', 'lemma': 'say', 'pos': 'VERB', 'tag': 'VBG', 'dep': 'ccomp', 'is_stop': False, 'is_alpha': True}, {'token': 'several', 'lemma': 'several', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': True, 'is_alpha': True}, {'token': 'locations', 'lemma': 'location', 'pos': 'NOUN', 'tag': 'NNS', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'like', 'lemma': 'like', 'pos': 'ADP', 'tag': 'IN', 'dep': 'prep', 'is_stop': False, 'is_alpha': True}, {'token': 'fries', 'lemma': 'fry', 'pos': 'NOUN', 'tag': 'NNS', 'dep': 'pobj', 'is_stop': False, 'is_alpha': True}, {'token': 'crispy', 'lemma': 'crispy', 'pos': 'VERB', 'tag': 'VBP', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'ask', 'lemma': 'ask', 'pos': 'VERB', 'tag': 'VB', 'dep': 'ccomp', 'is_stop': False, 'is_alpha': True}, {'token': 'well', 'lemma': 'well', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': True, 'is_alpha': True}, {'token': 'done', 'lemma': 'do', 'pos': 'VERB', 'tag': 'VBN', 'dep': 'dep', 'is_stop': True, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'extra', 'lemma': 'extra', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': 'crispy', 'lemma': 'crispy', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'scorched', 'lemma': 'scorch', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'ccomp', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'tortured', 'lemma': 'torture', 'pos': 'VERB', 'tag': 'VBN', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': 'hollow', 'lemma': 'hollow', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': 'tubes', 'lemma': 'tube', 'pos': 'NOUN', 'tag': 'NNS', 'dep': 'dobj', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'whatever', 'lemma': 'whatever', 'pos': 'DET', 'tag': 'WDT', 'dep': 'det', 'is_stop': True, 'is_alpha': True}, {'token': 'buzz', 'lemma': 'buzz', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': 'word', 'lemma': 'word', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': 'well', 'lemma': 'well', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': True, 'is_alpha': True}, {'token': 'done', 'lemma': 'do', 'pos': 'VERB', 'tag': 'VBN', 'dep': 'conj', 'is_stop': True, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'location', 'lemma': 'location', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'comply', 'lemma': 'comply', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'dobj', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'every', 'lemma': 'every', 'pos': 'DET', 'tag': 'DT', 'dep': 'det', 'is_stop': True, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'guys', 'lemma': 'guy', 'pos': 'NOUN', 'tag': 'NNS', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': 'complied', 'lemma': 'comply', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'conj', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'one', 'lemma': 'one', 'pos': 'NUM', 'tag': 'CD', 'dep': 'nummod', 'is_stop': True, 'is_alpha': True}, {'token': 'tatum', 'lemma': 'tatum', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'dobj', 'is_stop': False, 'is_alpha': True}, {'token': 'shea', 'lemma': 'shea', 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'said', 'lemma': 'say', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'ccomp', 'is_stop': False, 'is_alpha': True}, {'token': 'corporate', 'lemma': 'corporate', 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': 'said', 'lemma': 'say', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'ccomp', 'is_stop': False, 'is_alpha': True}, {'token': 'cook', 'lemma': 'cook', 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'fries', 'lemma': 'fry', 'pos': 'NOUN', 'tag': 'NNS', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': 'way', 'lemma': 'way', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'put', 'lemma': 'put', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'ccomp', 'is_stop': True, 'is_alpha': True}, {'token': 'soggy', 'lemma': 'soggy', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': 'fries', 'lemma': 'fry', 'pos': 'NOUN', 'tag': 'NNS', 'dep': 'dobj', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'yes', 'lemma': 'yes', 'pos': 'INTJ', 'tag': 'UH', 'dep': 'intj', 'is_stop': False, 'is_alpha': True}, {'token': 'soggy', 'lemma': 'soggy', 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'want', 'lemma': 'want', 'pos': 'AUX', 'tag': 'VBP', 'dep': 'aux', 'is_stop': False, 'is_alpha': True}, {'token': ' \\n\\n ', 'lemma': ' \\n\\n ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'also', 'lemma': 'also', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': True, 'is_alpha': True}, {'token': 'interrupted', 'lemma': 'interrupt', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'ccomp', 'is_stop': False, 'is_alpha': True}, {'token': 'us', 'lemma': 'we', 'pos': 'PRON', 'tag': 'PRP', 'dep': 'dobj', 'is_stop': True, 'is_alpha': True}, {'token': 'several', 'lemma': 'several', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': True, 'is_alpha': True}, {'token': 'times', 'lemma': 'time', 'pos': 'NOUN', 'tag': 'NNS', 'dep': 'npadvmod', 'is_stop': False, 'is_alpha': True}, {'token': 'rude', 'lemma': 'rude', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'nsubjpass', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'went', 'lemma': 'went', 'pos': 'AUX', 'tag': 'VBD', 'dep': 'auxpass', 'is_stop': False, 'is_alpha': True}, {'token': 'called', 'lemma': 'call', 'pos': 'VERB', 'tag': 'VBN', 'dep': 'ccomp', 'is_stop': False, 'is_alpha': True}, {'token': 'corporate', 'lemma': 'corporate', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': 'double', 'lemma': 'double', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': 'check', 'lemma': 'check', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'oprd', 'is_stop': False, 'is_alpha': True}, {'token': 'us', 'lemma': 'we', 'pos': 'PRON', 'tag': 'PRP', 'dep': 'nsubj', 'is_stop': True, 'is_alpha': True}, {'token': 'came', 'lemma': 'come', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'ccomp', 'is_stop': False, 'is_alpha': True}, {'token': 'table', 'lemma': 'table', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'dobj', 'is_stop': False, 'is_alpha': True}, {'token': 'said', 'lemma': 'say', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'ROOT', 'is_stop': False, 'is_alpha': True}, {'token': 'said', 'lemma': 'say', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'ccomp', 'is_stop': False, 'is_alpha': True}, {'token': 'cook', 'lemma': 'cook', 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'way', 'lemma': 'way', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'npadvmod', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'seriously', 'lemma': 'seriously', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'ask', 'lemma': 'ask', 'pos': 'VERB', 'tag': 'VB', 'dep': 'ccomp', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'actually', 'lemma': 'actually', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': False, 'is_alpha': True}, {'token': 'accused', 'lemma': 'accuse', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'xcomp', 'is_stop': False, 'is_alpha': True}, {'token': 'us', 'lemma': 'we', 'pos': 'PRON', 'tag': 'PRP', 'dep': 'dative', 'is_stop': True, 'is_alpha': True}, {'token': 'undercover', 'lemma': 'undercover', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': 'shoppers', 'lemma': 'shopper', 'pos': 'NOUN', 'tag': 'NNS', 'dep': 'dobj', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'started', 'lemma': 'start', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'ccomp', 'is_stop': False, 'is_alpha': True}, {'token': 'say', 'lemma': 'say', 'pos': 'VERB', 'tag': 'VBP', 'dep': 'xcomp', 'is_stop': True, 'is_alpha': True}, {'token': 'something', 'lemma': 'something', 'pos': 'PRON', 'tag': 'NN', 'dep': 'nsubj', 'is_stop': True, 'is_alpha': True}, {'token': 'interupted', 'lemma': 'interupte', 'pos': 'VERB', 'tag': 'VBN', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': ' \\n\\n ', 'lemma': ' \\n\\n ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'listen', 'lemma': 'listen', 'pos': 'VERB', 'tag': 'VB', 'dep': 'ccomp', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'explain', 'lemma': 'explain', 'pos': 'VERB', 'tag': 'VB', 'dep': 'ccomp', 'is_stop': False, 'is_alpha': True}, {'token': 'choice', 'lemma': 'choice', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'company', 'lemma': 'company', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'dobj', 'is_stop': False, 'is_alpha': True}, {'token': 'wishes', 'lemma': 'wish', 'pos': 'VERB', 'tag': 'VBZ', 'dep': 'ccomp', 'is_stop': False, 'is_alpha': True}, {'token': 'present', 'lemma': 'present', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': 'product', 'lemma': 'product', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': 'still', 'lemma': 'still', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': True, 'is_alpha': True}, {'token': 'choose', 'lemma': 'choose', 'pos': 'VERB', 'tag': 'VBP', 'dep': 'ccomp', 'is_stop': False, 'is_alpha': True}, {'token': 'different', 'lemma': 'different', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': 'way', 'lemma': 'way', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'dobj', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'comply', 'lemma': 'comply', 'pos': 'VERB', 'tag': 'VB', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'money', 'lemma': 'money', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'decision', 'lemma': 'decision', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'raised', 'lemma': 'raise', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'ROOT', 'is_stop': False, 'is_alpha': True}, {'token': 'rules', 'lemma': 'rule', 'pos': 'NOUN', 'tag': 'NNS', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': '  ', 'lemma': '  ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'customer', 'lemma': 'customer', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'dobj', 'is_stop': False, 'is_alpha': True}, {'token': 'always', 'lemma': 'always', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': True, 'is_alpha': True}, {'token': 'right', 'lemma': 'right', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': '   ', 'lemma': '   ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'customer', 'lemma': 'customer', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nmod', 'is_stop': False, 'is_alpha': True}, {'token': 'wrong', 'lemma': 'wrong', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': 'refer', 'lemma': 'refer', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'rule', 'lemma': 'rule', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'number', 'lemma': 'number', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': '   \\n\\n ', 'lemma': '   \\n\\n ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'return', 'lemma': 'return', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'npadvmod', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'lost', 'lemma': 'lose', 'pos': 'VERB', 'tag': 'VBN', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': 'business', 'lemma': 'business', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'hope', 'lemma': 'hope', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': 'loses', 'lemma': 'lose', 'pos': 'VERB', 'tag': 'VBZ', 'dep': 'conj', 'is_stop': False, 'is_alpha': True}, {'token': 'job', 'lemma': 'job', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': ' \\n ', 'lemma': ' \\n ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'want', 'lemma': 'want', 'pos': 'VERB', 'tag': 'VBP', 'dep': 'ccomp', 'is_stop': False, 'is_alpha': True}, {'token': 'try', 'lemma': 'try', 'pos': 'VERB', 'tag': 'VB', 'dep': 'conj', 'is_stop': False, 'is_alpha': True}, {'token': 'really', 'lemma': 'really', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': True, 'is_alpha': True}, {'token': 'good', 'lemma': 'good', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': 'burger', 'lemma': 'burger', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'fries', 'lemma': 'fry', 'pos': 'NOUN', 'tag': 'NNS', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'place', 'lemma': 'place', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': 'go', 'lemma': 'go', 'pos': 'VERB', 'tag': 'VBP', 'dep': 'ccomp', 'is_stop': True, 'is_alpha': True}, {'token': 'paradise', 'lemma': 'paradise', 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'valley', 'lemma': 'valley', 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'burger', 'lemma': 'burger', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'company', 'lemma': 'company', 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'dobj', 'is_stop': False, 'is_alpha': True}, {'token': 'th', 'lemma': 'th', 'pos': 'X', 'tag': 'FW', 'dep': 'det', 'is_stop': False, 'is_alpha': True}, {'token': 'street', 'lemma': 'street', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'bell', 'lemma': 'bell', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'disappointed', 'lemma': 'disappoint', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'conj', 'is_stop': False, 'is_alpha': True}]",
                        "count": 2
                      },
                      {
                        "name": "9995 others",
                        "count": 9996
                      }
                    ]
                  }
                },
                {
                  "name": "_deepnote_index_column",
                  "dtype": "int64"
                }
              ],
              "rows": [
                {
                  "stars": 5,
                  "text": "My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\n\nDo yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\n\nWhile EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\n\nAnyway, I can't wait to go back!",
                  "tokenized_text": "['wife', 'took', 'birthday', 'breakfast', 'excellent', '', ' ', 'weather', 'perfect', 'made', 'sitting', 'outside', 'overlooking', 'grounds', 'absolute', 'pleasure', '', ' ', 'waitress', 'excellent', 'food', 'arrived', 'quickly', 'semi', '', 'busy', 'saturday', 'morning', '', ' ', 'looked', 'like', 'place', 'fills', 'pretty', 'quickly', 'earlier', 'get', 'better', '', '\\n\\n', 'favor', 'get', 'bloody', 'mary', '', ' ', 'phenomenal', 'simply', 'best', 'ever', '', ' ', 'pretty', 'sure', 'use', 'ingredients', 'garden', 'blend', 'fresh', 'order', '', ' ', 'amazing', '', '\\n\\n', 'everything', 'menu', 'looks', 'excellent', '', 'white', 'truffle', 'scrambled', 'eggs', 'vegetable', 'skillet', 'tasty', 'delicious', '', ' ', 'came', '', 'pieces', 'griddled', 'bread', 'amazing', 'absolutely', 'made', 'meal', 'complete', '', ' ', 'best', '', 'toast', '', 'ever', '', '\\n\\n', 'anyway', '', 'ca', 'nt', 'wait', 'go', 'back', '']",
                  "lemmatized_text": "['my', 'wife', 'take', 'I', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'be', 'excellent', '.', ' ', 'the', 'weather', 'be', 'perfect', 'which', 'make', 'sit', 'outside', 'overlook', 'their', 'ground', 'an', 'absolute', 'pleasure', '.', ' ', 'our', 'waitress', 'be', 'excellent', 'and', 'our', 'food', 'arrive', 'quickly', 'on', 'the', 'semi', '-', 'busy', 'Saturday', 'morning', '.', ' ', 'it', 'look', 'like', 'the', 'place', 'fill', 'up', 'pretty', 'quickly', 'so', 'the', 'early', 'you', 'get', 'here', 'the', 'well', '.', '\\n\\n', 'do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', '.', ' ', 'it', 'be', 'phenomenal', 'and', 'simply', 'the', 'good', 'I', \"'ve\", 'ever', 'have', '.', ' ', 'I', 'be', 'pretty', 'sure', 'they', 'only', 'use', 'ingredient', 'from', 'their', 'garden', 'and', 'blend', 'they', 'fresh', 'when', 'you', 'order', 'it', '.', ' ', 'it', 'be', 'amazing', '.', '\\n\\n', 'while', 'everything', 'on', 'the', 'menu', 'look', 'excellent…",
                  "linguistic_features": "[{'token': 'wife', 'lemma': 'wife', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': 'took', 'lemma': 'take', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'ROOT', 'is_stop': False, 'is_alpha': True}, {'token': 'birthday', 'lemma': 'birthday', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'breakfast', 'lemma': 'breakfast', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'excellent', 'lemma': 'excellent', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': '   ', 'lemma': '   ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'weather', 'lemma': 'weather', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'perfect', 'lemma': 'perfect', 'pos': 'ADV', 'tag': 'RB', 'dep': 'dobj', 'is_stop': False, 'is_alpha': True}, {'token': 'made', 'lemma':…",
                  "_deepnote_index_column": 0
                },
                {
                  "stars": 5,
                  "text": "I have no idea why some people give bad reviews about this place. It goes to show you, you can please everyone. They are probably griping about something that their own fault...there are many people like that.\n\nIn any case, my friend and I arrived at about 5:50 PM this past Sunday. It was pretty crowded, more than I thought for a Sunday evening and thought we would have to wait forever to get a seat but they said we'll be seated when the girl comes back from seating someone else. We were seated at 5:52 and the waiter came and got our drink orders. Everyone was very pleasant from the host that seated us to the waiter to the server. The prices were very good as well. We placed our orders once we decided what we wanted at 6:02. We shared the baked spaghetti calzone and the small \"Here's The Beef\" pizza so we can both try them. The calzone was huge and we got the smallest one (personal) and got the small 11\" pizza. Both were awesome! My friend liked the pizza better and I liked the calzon…",
                  "tokenized_text": "['idea', 'people', 'give', 'bad', 'reviews', 'place', '', 'goes', 'show', '', 'please', 'everyone', '', 'probably', 'griping', 'something', 'fault', '', 'many', 'people', 'like', '', '\\n\\n', 'case', '', 'friend', 'arrived', '', 'pm', 'past', 'sunday', '', 'pretty', 'crowded', '', 'thought', 'sunday', 'evening', 'thought', 'would', 'wait', 'forever', 'get', 'seat', 'said', 'seated', 'girl', 'comes', 'back', 'seating', 'someone', 'else', '', 'seated', '', 'waiter', 'came', 'got', 'drink', 'orders', '', 'everyone', 'pleasant', 'host', 'seated', 'us', 'waiter', 'server', '', 'prices', 'good', 'well', '', 'placed', 'orders', 'decided', 'wanted', '', '', 'shared', 'baked', 'spaghetti', 'calzone', 'small', '', 'beef', '', 'pizza', 'try', '', 'calzone', 'huge', 'got', 'smallest', 'one', '', 'personal', '', 'got', 'small', '', '', 'pizza', '', 'awesome', '', 'friend', 'liked', 'pizza', 'better', 'liked', 'calzone', 'better', '', 'calzone', 'sweetish', 'sauce', 'like', 'sauce', '', '\\n\\n', 'box…",
                  "lemmatized_text": "['I', 'have', 'no', 'idea', 'why', 'some', 'people', 'give', 'bad', 'review', 'about', 'this', 'place', '.', 'it', 'go', 'to', 'show', 'you', ',', 'you', 'can', 'please', 'everyone', '.', 'they', 'be', 'probably', 'gripe', 'about', 'something', 'that', 'their', 'own', 'fault', '...', 'there', 'be', 'many', 'people', 'like', 'that', '.', '\\n\\n', 'in', 'any', 'case', ',', 'my', 'friend', 'and', 'I', 'arrive', 'at', 'about', '5:50', 'pm', 'this', 'past', 'Sunday', '.', 'it', 'be', 'pretty', 'crowded', ',', 'more', 'than', 'I', 'think', 'for', 'a', 'Sunday', 'evening', 'and', 'think', 'we', 'would', 'have', 'to', 'wait', 'forever', 'to', 'get', 'a', 'seat', 'but', 'they', 'say', 'we', 'will', 'be', 'seat', 'when', 'the', 'girl', 'come', 'back', 'from', 'seat', 'someone', 'else', '.', 'we', 'be', 'seat', 'at', '5:52', 'and', 'the', 'waiter', 'come', 'and', 'get', 'our', 'drink', 'order', '.', 'everyone', 'be', 'very', 'pleasant', 'from', 'the', 'host', 'that', 'seat', 'we', 'to', 'the', 'w…",
                  "linguistic_features": "[{'token': 'idea', 'lemma': 'idea', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'people', 'lemma': 'people', 'pos': 'NOUN', 'tag': 'NNS', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': 'give', 'lemma': 'give', 'pos': 'VERB', 'tag': 'VBP', 'dep': 'ccomp', 'is_stop': True, 'is_alpha': True}, {'token': 'bad', 'lemma': 'bad', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': 'reviews', 'lemma': 'review', 'pos': 'NOUN', 'tag': 'NNS', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'place', 'lemma': 'place', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'dobj', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'goes', 'lemma': 'go', 'pos': 'VERB', 'tag': 'VBZ', 'dep': 'conj', 'is_stop': False, 'is_alpha': True}, {'token': 'show', 'lemma': 'show', 'pos': 'NOUN', 'tag': 'NN', …",
                  "_deepnote_index_column": 1
                },
                {
                  "stars": 4,
                  "text": "love the gyro plate. Rice is so good and I also dig their candy selection :)",
                  "tokenized_text": "['love', 'gyro', 'plate', '', 'rice', 'good', 'also', 'dig', 'candy', 'selection', '']",
                  "lemmatized_text": "['love', 'the', 'gyro', 'plate', '.', 'rice', 'be', 'so', 'good', 'and', 'I', 'also', 'dig', 'their', 'candy', 'selection', ':)']",
                  "linguistic_features": "[{'token': 'love', 'lemma': 'love', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'gyro', 'lemma': 'gyro', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'plate', 'lemma': 'plate', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'rice', 'lemma': 'rice', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'good', 'lemma': 'good', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': 'also', 'lemma': 'also', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': True, 'is_alpha': True}, {'token': 'dig', 'lemma': 'dig', 'pos': 'VERB', 'tag': 'VBP', 'dep': 'ROOT', 'is_stop': False, 'is_alpha': True}, {'token': 'candy', 'lemma': 'candy', 'pos': 'NOUN', 'tag': 'NN',…",
                  "_deepnote_index_column": 2
                },
                {
                  "stars": 5,
                  "text": "Rosie, Dakota, and I LOVE Chaparral Dog Park!!! It's very convenient and surrounded by a lot of paths, a desert xeriscape, baseball fields, ballparks, and a lake with ducks.\n\nThe Scottsdale Park and Rec Dept. does a wonderful job of keeping the park clean and shaded.  You can find trash cans and poopy-pick up mitts located all over the park and paths.\n\nThe fenced in area is huge to let the dogs run, play, and sniff!",
                  "tokenized_text": "['rosie', '', 'dakota', '', 'love', 'chaparral', 'dog', 'park', '', '', '', 'convenient', 'surrounded', 'lot', 'paths', '', 'desert', 'xeriscape', '', 'baseball', 'fields', '', 'ballparks', '', 'lake', 'ducks', '', '\\n\\n', 'scottsdale', 'park', 'rec', 'dept', '', 'wonderful', 'job', 'keeping', 'park', 'clean', 'shaded', '', ' ', 'find', 'trash', 'cans', 'poopy', '', 'pick', 'mitts', 'located', 'park', 'paths', '', '\\n\\n', 'fenced', 'area', 'huge', 'let', 'dogs', 'run', '', 'play', '', 'sniff', '']",
                  "lemmatized_text": "['Rosie', ',', 'Dakota', ',', 'and', 'I', 'love', 'Chaparral', 'Dog', 'Park', '!', '!', '!', 'it', 'be', 'very', 'convenient', 'and', 'surround', 'by', 'a', 'lot', 'of', 'path', ',', 'a', 'desert', 'xeriscape', ',', 'baseball', 'field', ',', 'ballpark', ',', 'and', 'a', 'lake', 'with', 'duck', '.', '\\n\\n', 'the', 'Scottsdale', 'Park', 'and', 'Rec', 'Dept', '.', 'do', 'a', 'wonderful', 'job', 'of', 'keep', 'the', 'park', 'clean', 'and', 'shaded', '.', ' ', 'you', 'can', 'find', 'trash', 'can', 'and', 'poopy', '-', 'pick', 'up', 'mitt', 'locate', 'all', 'over', 'the', 'park', 'and', 'path', '.', '\\n\\n', 'the', 'fenced', 'in', 'area', 'be', 'huge', 'to', 'let', 'the', 'dog', 'run', ',', 'play', ',', 'and', 'sniff', '!']",
                  "linguistic_features": "[{'token': 'rosie', 'lemma': 'rosie', 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'dakota', 'lemma': 'dakota', 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'love', 'lemma': 'love', 'pos': 'VERB', 'tag': 'VBP', 'dep': 'nmod', 'is_stop': False, 'is_alpha': True}, {'token': 'chaparral', 'lemma': 'chaparral', 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': 'dog', 'lemma': 'dog', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'park', 'lemma': 'park', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': '   ', 'lemma': '   ', 'pos': 'SPACE', 'ta…",
                  "_deepnote_index_column": 3
                },
                {
                  "stars": 5,
                  "text": "General Manager Scott Petello is a good egg!!! Not to go into detail, but let me assure you if you have any issues (albeit rare) speak with Scott and treat the guy with some respect as you state your case and I'd be surprised if you don't walk out totally satisfied as I just did. Like I always say..... \"Mistakes are inevitable, it's how we recover from them that is important\"!!!\n\nThanks to Scott and his awesome staff. You've got a customer for life!! .......... :^)",
                  "tokenized_text": "['general', 'manager', 'scott', 'petello', 'good', 'egg', '', '', '', 'go', 'detail', '', 'let', 'assure', 'issues', '', 'albeit', 'rare', '', 'speak', 'scott', 'treat', 'guy', 'respect', 'state', 'case', 'surprised', 'nt', 'walk', 'totally', 'satisfied', '', 'like', 'always', 'say', '', '', 'mistakes', 'inevitable', '', 'recover', 'important', '', '', '', '', '\\n\\n', 'thanks', 'scott', 'awesome', 'staff', '', 'got', 'customer', 'life', '', '', '', '', '', '']",
                  "lemmatized_text": "['General', 'Manager', 'Scott', 'Petello', 'be', 'a', 'good', 'egg', '!', '!', '!', 'not', 'to', 'go', 'into', 'detail', ',', 'but', 'let', 'I', 'assure', 'you', 'if', 'you', 'have', 'any', 'issue', '(', 'albeit', 'rare', ')', 'speak', 'with', 'Scott', 'and', 'treat', 'the', 'guy', 'with', 'some', 'respect', 'as', 'you', 'state', 'your', 'case', 'and', 'I', 'would', 'be', 'surprised', 'if', 'you', 'do', 'not', 'walk', 'out', 'totally', 'satisfied', 'as', 'I', 'just', 'do', '.', 'like', 'I', 'always', 'say', '.....', '\"', 'mistake', 'be', 'inevitable', ',', 'it', 'be', 'how', 'we', 'recover', 'from', 'they', 'that', 'be', 'important', '\"', '!', '!', '!', '\\n\\n', 'thank', 'to', 'Scott', 'and', 'his', 'awesome', 'staff', '.', 'you', \"'ve\", 'get', 'a', 'customer', 'for', 'life', '!', '!', '..........', ':', '^', ')']",
                  "linguistic_features": "[{'token': 'general', 'lemma': 'general', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': 'manager', 'lemma': 'manager', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': 'scott', 'lemma': 'scott', 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': 'petello', 'lemma': 'petello', 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': 'good', 'lemma': 'good', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': 'egg', 'lemma': 'egg', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'dobj', 'is_stop': False, 'is_alpha': True}, {'token': '   ', 'lemma': '   ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'go', 'lemma': 'go', 'pos': 'VERB', 'tag': 'VB', 'dep': 'ROOT', 'is_stop': True, 'is_alpha': True}, {'token': 'detail', 'lemma': 'detail', 'pos': 'NOUN', 'ta…",
                  "_deepnote_index_column": 4
                },
                {
                  "stars": 4,
                  "text": "Quiessence is, simply put, beautiful.  Full windows and earthy wooden walls give a feeling of warmth inside this restaurant perched in the middle of a farm.  The restaurant seemed fairly full even on a Tuesday evening; we had secured reservations just a couple days before.\n\nMy friend and I had sampled sandwiches at the Farm Kitchen earlier that week, and were impressed enough to want to eat at the restaurant.  The crisp, fresh veggies didn't disappoint: we ordered the salad with orange and grapefruit slices and the crudites to start.  Both were very good; I didn't even know how much I liked raw radishes and turnips until I tried them with their pesto and aioli sauces.\n\nFor entrees, I ordered the lamb and my friend ordered the pork shoulder.  Service started out very good, but trailed off quickly.  Waiting for our food took a very long time (a couple seated after us received and finished their entrees before we received our's), and no one bothered to explain the situation until the mai…",
                  "tokenized_text": "['quiessence', '', 'simply', 'put', '', 'beautiful', '', ' ', 'full', 'windows', 'earthy', 'wooden', 'walls', 'give', 'feeling', 'warmth', 'inside', 'restaurant', 'perched', 'middle', 'farm', '', ' ', 'restaurant', 'seemed', 'fairly', 'full', 'even', 'tuesday', 'evening', '', 'secured', 'reservations', 'couple', 'days', '', '\\n\\n', 'friend', 'sampled', 'sandwiches', 'farm', 'kitchen', 'earlier', 'week', '', 'impressed', 'enough', 'want', 'eat', 'restaurant', '', ' ', 'crisp', '', 'fresh', 'veggies', 'nt', 'disappoint', '', 'ordered', 'salad', 'orange', 'grapefruit', 'slices', 'crudites', 'start', '', ' ', 'good', '', 'nt', 'even', 'know', 'much', 'liked', 'raw', 'radishes', 'turnips', 'tried', 'pesto', 'aioli', 'sauces', '', '\\n\\n', 'entrees', '', 'ordered', 'lamb', 'friend', 'ordered', 'pork', 'shoulder', '', ' ', 'service', 'started', 'good', '', 'trailed', 'quickly', '', ' ', 'waiting', 'food', 'took', 'long', 'time', '', 'couple', 'seated', 'us', 'received', 'finished', 'entrees',…",
                  "lemmatized_text": "['Quiessence', 'be', ',', 'simply', 'put', ',', 'beautiful', '.', ' ', 'full', 'window', 'and', 'earthy', 'wooden', 'wall', 'give', 'a', 'feeling', 'of', 'warmth', 'inside', 'this', 'restaurant', 'perch', 'in', 'the', 'middle', 'of', 'a', 'farm', '.', ' ', 'the', 'restaurant', 'seem', 'fairly', 'full', 'even', 'on', 'a', 'Tuesday', 'evening', ';', 'we', 'have', 'secure', 'reservation', 'just', 'a', 'couple', 'day', 'before', '.', '\\n\\n', 'my', 'friend', 'and', 'I', 'have', 'sample', 'sandwich', 'at', 'the', 'Farm', 'Kitchen', 'early', 'that', 'week', ',', 'and', 'be', 'impress', 'enough', 'to', 'want', 'to', 'eat', 'at', 'the', 'restaurant', '.', ' ', 'the', 'crisp', ',', 'fresh', 'veggie', 'do', 'not', 'disappoint', ':', 'we', 'order', 'the', 'salad', 'with', 'orange', 'and', 'grapefruit', 'slice', 'and', 'the', 'crudite', 'to', 'start', '.', ' ', 'both', 'be', 'very', 'good', ';', 'I', 'do', 'not', 'even', 'know', 'how', 'much', 'I', 'like', 'raw', 'radish', 'and', 'turnip', 'until'…",
                  "linguistic_features": "[{'token': 'quiessence', 'lemma': 'quiessence', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'simply', 'lemma': 'simply', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': False, 'is_alpha': True}, {'token': 'put', 'lemma': 'put', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'ccomp', 'is_stop': True, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'beautiful', 'lemma': 'beautiful', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': '   ', 'lemma': '   ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'full', 'lemma': 'full', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': True, 'is_alpha': True}, {'token': 'windows', 'lemma': 'windows', 'pos': 'PROPN', 'tag'…",
                  "_deepnote_index_column": 5
                },
                {
                  "stars": 5,
                  "text": "Drop what you're doing and drive here. After I ate here I had to go back the next day for more.  The food is that good.\n\nThis cute little green building may have gone competely unoticed if I hadn't been driving down Palm Rd to avoid construction.  While waiting to turn onto 16th Street the \"Grand Opening\" sign caught my eye and my little yelping soul leaped for joy!  A new place to try!\n\nIt looked desolate from the outside but when I opened the door I was put at easy by the decor, smell and cleanliness inside.  I ordered dinner for two, to go.  The menu was awesome.  I loved seeing all the variety: poblano peppers, mole, mahi mahi, mushrooms...something wrapped in banana leaves.  It made it difficult to choose something.  Here's what I've had so far: La Condesa Shrimp Burro and Baja Sur Dogfish Shark Taco.  They are both were very delicious meals but the shrimp burro stole the show.  So much flavor.  I snagged some bites from my hubbys mole and mahi mahi burros- mmmm such a delight.  …",
                  "tokenized_text": "['drop', 'drive', '', 'ate', 'go', 'back', 'next', 'day', '', ' ', 'food', 'good', '', '\\n\\n', 'cute', 'little', 'green', 'building', 'may', 'gone', 'competely', 'unoticed', 'nt', 'driving', 'palm', 'rd', 'avoid', 'construction', '', ' ', 'waiting', 'turn', 'onto', 'th', 'street', '', 'grand', 'opening', '', 'sign', 'caught', 'eye', 'little', 'yelping', 'soul', 'leaped', 'joy', '', ' ', 'new', 'place', 'try', '', '\\n\\n', 'looked', 'desolate', 'outside', 'opened', 'door', 'put', 'easy', 'decor', '', 'smell', 'cleanliness', 'inside', '', ' ', 'ordered', 'dinner', 'two', '', 'go', '', ' ', 'menu', 'awesome', '', ' ', 'loved', 'seeing', 'variety', '', 'poblano', 'peppers', '', 'mole', '', 'mahi', 'mahi', '', 'mushrooms', '', 'something', 'wrapped', 'banana', 'leaves', '', ' ', 'made', 'difficult', 'choose', 'something', '', ' ', 'far', '', 'la', 'condesa', 'shrimp', 'burro', 'baja', 'sur', 'dogfish', 'shark', 'taco', '', ' ', 'delicious', 'meals', 'shrimp', 'burro', 'stole', 'show', '', '…",
                  "lemmatized_text": "['drop', 'what', 'you', 'be', 'do', 'and', 'drive', 'here', '.', 'after', 'I', 'eat', 'here', 'I', 'have', 'to', 'go', 'back', 'the', 'next', 'day', 'for', 'more', '.', ' ', 'the', 'food', 'be', 'that', 'good', '.', '\\n\\n', 'this', 'cute', 'little', 'green', 'building', 'may', 'have', 'go', 'competely', 'unoticed', 'if', 'I', 'have', 'not', 'be', 'drive', 'down', 'Palm', 'Rd', 'to', 'avoid', 'construction', '.', ' ', 'while', 'wait', 'to', 'turn', 'onto', '16th', 'Street', 'the', '\"', 'Grand', 'Opening', '\"', 'sign', 'catch', 'my', 'eye', 'and', 'my', 'little', 'yelp', 'soul', 'leap', 'for', 'joy', '!', ' ', 'a', 'new', 'place', 'to', 'try', '!', '\\n\\n', 'it', 'look', 'desolate', 'from', 'the', 'outside', 'but', 'when', 'I', 'open', 'the', 'door', 'I', 'be', 'put', 'at', 'easy', 'by', 'the', 'decor', ',', 'smell', 'and', 'cleanliness', 'inside', '.', ' ', 'I', 'order', 'dinner', 'for', 'two', ',', 'to', 'go', '.', ' ', 'the', 'menu', 'be', 'awesome', '.', ' ', 'I', 'love', 'see', 'all…",
                  "linguistic_features": "[{'token': 'drop', 'lemma': 'drop', 'pos': 'VERB', 'tag': 'VB', 'dep': 'intj', 'is_stop': False, 'is_alpha': True}, {'token': 'drive', 'lemma': 'drive', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'ate', 'lemma': 'eat', 'pos': 'VERB', 'tag': 'VBD', 'dep': 'acl', 'is_stop': False, 'is_alpha': True}, {'token': 'go', 'lemma': 'go', 'pos': 'VERB', 'tag': 'VB', 'dep': 'ROOT', 'is_stop': True, 'is_alpha': True}, {'token': 'back', 'lemma': 'back', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': True, 'is_alpha': True}, {'token': 'next', 'lemma': 'next', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': True, 'is_alpha': True}, {'token': 'day', 'lemma': 'day', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'npadvmod', 'is_stop': False, 'is_alpha': True}, {'token': '   ', 'lemma': '   ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop'…",
                  "_deepnote_index_column": 6
                },
                {
                  "stars": 4,
                  "text": "Luckily, I didn't have to travel far to make my connecting flight. And for this, I thank you, Phoenix.\n\nMy brief layover was pleasant as the employees were kind and the flight was on time.  Hopefully, next time I can grace Phoenix with my presence for a little while longer.",
                  "tokenized_text": "['luckily', '', 'nt', 'travel', 'far', 'make', 'connecting', 'flight', '', '', 'thank', '', 'phoenix', '', '\\n\\n', 'brief', 'layover', 'pleasant', 'employees', 'kind', 'flight', 'time', '', ' ', 'hopefully', '', 'next', 'time', 'grace', 'phoenix', 'presence', 'little', 'longer', '']",
                  "lemmatized_text": "['luckily', ',', 'I', 'do', 'not', 'have', 'to', 'travel', 'far', 'to', 'make', 'my', 'connect', 'flight', '.', 'and', 'for', 'this', ',', 'I', 'thank', 'you', ',', 'Phoenix', '.', '\\n\\n', 'my', 'brief', 'layover', 'be', 'pleasant', 'as', 'the', 'employee', 'be', 'kind', 'and', 'the', 'flight', 'be', 'on', 'time', '.', ' ', 'hopefully', ',', 'next', 'time', 'I', 'can', 'grace', 'Phoenix', 'with', 'my', 'presence', 'for', 'a', 'little', 'while', 'long', '.']",
                  "linguistic_features": "[{'token': 'luckily', 'lemma': 'luckily', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'nt', 'lemma': 'not', 'pos': 'PART', 'tag': 'RB', 'dep': 'advmod', 'is_stop': False, 'is_alpha': True}, {'token': 'travel', 'lemma': 'travel', 'pos': 'VERB', 'tag': 'VBP', 'dep': 'ccomp', 'is_stop': False, 'is_alpha': True}, {'token': 'far', 'lemma': 'far', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': False, 'is_alpha': True}, {'token': 'make', 'lemma': 'make', 'pos': 'VERB', 'tag': 'VBP', 'dep': 'advcl', 'is_stop': True, 'is_alpha': True}, {'token': 'connecting', 'lemma': 'connect', 'pos': 'VERB', 'tag': 'VBG', 'dep': 'xcomp', 'is_stop': False, 'is_alpha': True}, {'token': 'flight', 'lemma': 'flight', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'dobj', 'is_stop': False, 'is_alpha': True}, {'token': '  ', 'lemma': '  ', 'pos': 'SPACE', 'tag': '…",
                  "_deepnote_index_column": 7
                },
                {
                  "stars": 4,
                  "text": "Definitely come for Happy hour! Prices are amazing, sake bombers for $3...Great atmosphere and wait staff was incredibly nice and right on to all of our needs, didn't have to ask for a thing They were always spot on...Place gets crowded in the evening especially if you plan on sitting outside. I only wish there were one in Apollo Beach or Brandon!",
                  "tokenized_text": "['definitely', 'come', 'happy', 'hour', '', 'prices', 'amazing', '', 'sake', 'bombers', '', '', '', 'great', 'atmosphere', 'wait', 'staff', 'incredibly', 'nice', 'right', 'needs', '', 'nt', 'ask', 'thing', 'always', 'spot', '', 'place', 'gets', 'crowded', 'evening', 'especially', 'plan', 'sitting', 'outside', '', 'wish', 'one', 'apollo', 'beach', 'brandon', '']",
                  "lemmatized_text": "['definitely', 'come', 'for', 'happy', 'hour', '!', 'price', 'be', 'amazing', ',', 'sake', 'bomber', 'for', '$', '3', '...', 'great', 'atmosphere', 'and', 'wait', 'staff', 'be', 'incredibly', 'nice', 'and', 'right', 'on', 'to', 'all', 'of', 'our', 'need', ',', 'do', 'not', 'have', 'to', 'ask', 'for', 'a', 'thing', 'they', 'be', 'always', 'spot', 'on', '...', 'Place', 'gets', 'crowd', 'in', 'the', 'evening', 'especially', 'if', 'you', 'plan', 'on', 'sit', 'outside', '.', 'I', 'only', 'wish', 'there', 'be', 'one', 'in', 'Apollo', 'Beach', 'or', 'Brandon', '!']",
                  "linguistic_features": "[{'token': 'definitely', 'lemma': 'definitely', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': False, 'is_alpha': True}, {'token': 'come', 'lemma': 'come', 'pos': 'VERB', 'tag': 'VB', 'dep': 'ccomp', 'is_stop': False, 'is_alpha': True}, {'token': 'happy', 'lemma': 'happy', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': 'hour', 'lemma': 'hour', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'compound', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'prices', 'lemma': 'price', 'pos': 'NOUN', 'tag': 'NNS', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': 'amazing', 'lemma': 'amazing', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'advcl', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'sake', 'lemma': 'sake', 'pos': 'NOUN', 'tag':…",
                  "_deepnote_index_column": 8
                },
                {
                  "stars": 5,
                  "text": "Nobuo shows his unique talents with everything on the menu. Carefully crafted features with much to drink. Start with the pork belly buns and a stout. Then go on until you can no longer.",
                  "tokenized_text": "['nobuo', 'shows', 'unique', 'talents', 'everything', 'menu', '', 'carefully', 'crafted', 'features', 'much', 'drink', '', 'start', 'pork', 'belly', 'buns', 'stout', '', 'go', 'longer', '']",
                  "lemmatized_text": "['Nobuo', 'show', 'his', 'unique', 'talent', 'with', 'everything', 'on', 'the', 'menu', '.', 'carefully', 'craft', 'feature', 'with', 'much', 'to', 'drink', '.', 'start', 'with', 'the', 'pork', 'belly', 'bun', 'and', 'a', 'stout', '.', 'then', 'go', 'on', 'until', 'you', 'can', 'no', 'long', '.']",
                  "linguistic_features": "[{'token': 'nobuo', 'lemma': 'nobuo', 'pos': 'PROPN', 'tag': 'NNP', 'dep': 'nsubj', 'is_stop': False, 'is_alpha': True}, {'token': 'shows', 'lemma': 'show', 'pos': 'VERB', 'tag': 'VBZ', 'dep': 'ROOT', 'is_stop': False, 'is_alpha': True}, {'token': 'unique', 'lemma': 'unique', 'pos': 'ADJ', 'tag': 'JJ', 'dep': 'amod', 'is_stop': False, 'is_alpha': True}, {'token': 'talents', 'lemma': 'talent', 'pos': 'NOUN', 'tag': 'NNS', 'dep': 'dobj', 'is_stop': False, 'is_alpha': True}, {'token': 'everything', 'lemma': 'everything', 'pos': 'PRON', 'tag': 'NN', 'dep': 'compound', 'is_stop': True, 'is_alpha': True}, {'token': 'menu', 'lemma': 'menu', 'pos': 'NOUN', 'tag': 'NN', 'dep': 'appos', 'is_stop': False, 'is_alpha': True}, {'token': ' ', 'lemma': ' ', 'pos': 'SPACE', 'tag': '_SP', 'dep': 'dep', 'is_stop': False, 'is_alpha': False}, {'token': 'carefully', 'lemma': 'carefully', 'pos': 'ADV', 'tag': 'RB', 'dep': 'advmod', 'is_stop': False, 'is_alpha': True}, {'token': 'crafted', 'lemma': 'craft', …",
                  "_deepnote_index_column": 9
                }
              ]
            },
            "text/plain": "      stars                                               text  \\\n0         5  My wife took me here on my birthday for breakf...   \n1         5  I have no idea why some people give bad review...   \n2         4  love the gyro plate. Rice is so good and I als...   \n3         5  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...   \n4         5  General Manager Scott Petello is a good egg!!!...   \n...     ...                                                ...   \n9995      3  First visit...Had lunch here today - used my G...   \n9996      4  Should be called house of deliciousness!\\n\\nI ...   \n9997      4  I recently visited Olive and Ivy for business ...   \n9998      2  My nephew just moved to Scottsdale recently so...   \n9999      5  4-5 locations.. all 4.5 star average.. I think...   \n\n                                         tokenized_text  \\\n0     [wife, took, birthday, breakfast, excellent, ,...   \n1     [idea, people, give, bad, reviews, place, , go...   \n2     [love, gyro, plate, , rice, good, also, dig, c...   \n3     [rosie, , dakota, , love, chaparral, dog, park...   \n4     [general, manager, scott, petello, good, egg, ...   \n...                                                 ...   \n9995  [first, visit, , lunch, today, , used, groupon...   \n9996  [called, house, deliciousness, , \\n\\n, could, ...   \n9997  [recently, visited, olive, ivy, business, last...   \n9998  [nephew, moved, scottsdale, recently, bunch, f...   \n9999  [, , , locations, , , star, average, , think, ...   \n\n                                        lemmatized_text  \\\n0     [my, wife, take, I, here, on, my, birthday, fo...   \n1     [I, have, no, idea, why, some, people, give, b...   \n2     [love, the, gyro, plate, ., rice, be, so, good...   \n3     [Rosie, ,, Dakota, ,, and, I, love, Chaparral,...   \n4     [General, Manager, Scott, Petello, be, a, good...   \n...                                                 ...   \n9995  [first, visit, ..., have, lunch, here, today, ...   \n9996  [should, be, call, house, of, deliciousness, !...   \n9997  [I, recently, visit, Olive, and, Ivy, for, bus...   \n9998  [my, nephew, just, move, to, Scottsdale, recen...   \n9999  [4, -, 5, location, .., all, 4.5, star, averag...   \n\n                                    linguistic_features  \n0     [{'token': 'wife', 'lemma': 'wife', 'pos': 'NO...  \n1     [{'token': 'idea', 'lemma': 'idea', 'pos': 'NO...  \n2     [{'token': 'love', 'lemma': 'love', 'pos': 'NO...  \n3     [{'token': 'rosie', 'lemma': 'rosie', 'pos': '...  \n4     [{'token': 'general', 'lemma': 'general', 'pos...  \n...                                                 ...  \n9995  [{'token': 'first', 'lemma': 'first', 'pos': '...  \n9996  [{'token': 'called', 'lemma': 'call', 'pos': '...  \n9997  [{'token': 'recently', 'lemma': 'recently', 'p...  \n9998  [{'token': 'nephew', 'lemma': 'nephew', 'pos':...  \n9999  [{'token': '   ', 'lemma': '   ', 'pos': 'SPAC...  \n\n[10000 rows x 5 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stars</th>\n      <th>text</th>\n      <th>tokenized_text</th>\n      <th>lemmatized_text</th>\n      <th>linguistic_features</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>My wife took me here on my birthday for breakf...</td>\n      <td>[wife, took, birthday, breakfast, excellent, ,...</td>\n      <td>[my, wife, take, I, here, on, my, birthday, fo...</td>\n      <td>[{'token': 'wife', 'lemma': 'wife', 'pos': 'NO...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>I have no idea why some people give bad review...</td>\n      <td>[idea, people, give, bad, reviews, place, , go...</td>\n      <td>[I, have, no, idea, why, some, people, give, b...</td>\n      <td>[{'token': 'idea', 'lemma': 'idea', 'pos': 'NO...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>love the gyro plate. Rice is so good and I als...</td>\n      <td>[love, gyro, plate, , rice, good, also, dig, c...</td>\n      <td>[love, the, gyro, plate, ., rice, be, so, good...</td>\n      <td>[{'token': 'love', 'lemma': 'love', 'pos': 'NO...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n      <td>[rosie, , dakota, , love, chaparral, dog, park...</td>\n      <td>[Rosie, ,, Dakota, ,, and, I, love, Chaparral,...</td>\n      <td>[{'token': 'rosie', 'lemma': 'rosie', 'pos': '...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>General Manager Scott Petello is a good egg!!!...</td>\n      <td>[general, manager, scott, petello, good, egg, ...</td>\n      <td>[General, Manager, Scott, Petello, be, a, good...</td>\n      <td>[{'token': 'general', 'lemma': 'general', 'pos...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>3</td>\n      <td>First visit...Had lunch here today - used my G...</td>\n      <td>[first, visit, , lunch, today, , used, groupon...</td>\n      <td>[first, visit, ..., have, lunch, here, today, ...</td>\n      <td>[{'token': 'first', 'lemma': 'first', 'pos': '...</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>4</td>\n      <td>Should be called house of deliciousness!\\n\\nI ...</td>\n      <td>[called, house, deliciousness, , \\n\\n, could, ...</td>\n      <td>[should, be, call, house, of, deliciousness, !...</td>\n      <td>[{'token': 'called', 'lemma': 'call', 'pos': '...</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>4</td>\n      <td>I recently visited Olive and Ivy for business ...</td>\n      <td>[recently, visited, olive, ivy, business, last...</td>\n      <td>[I, recently, visit, Olive, and, Ivy, for, bus...</td>\n      <td>[{'token': 'recently', 'lemma': 'recently', 'p...</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>2</td>\n      <td>My nephew just moved to Scottsdale recently so...</td>\n      <td>[nephew, moved, scottsdale, recently, bunch, f...</td>\n      <td>[my, nephew, just, move, to, Scottsdale, recen...</td>\n      <td>[{'token': 'nephew', 'lemma': 'nephew', 'pos':...</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>5</td>\n      <td>4-5 locations.. all 4.5 star average.. I think...</td>\n      <td>[, , , locations, , , star, average, , think, ...</td>\n      <td>[4, -, 5, location, .., all, 4.5, star, averag...</td>\n      <td>[{'token': '   ', 'lemma': '   ', 'pos': 'SPAC...</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows × 5 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "1fcab994",
        "execution_start": 1686919794641,
        "execution_millis": 22,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "b5167c7736b5488b81633e92f24ea031",
        "deepnote_cell_type": "code",
        "id": "pMPsyFVQbe5j",
        "outputId": "b9298328-c5b6-42f6-da5e-446122ac1c22"
      },
      "source": [
        "corpus = data['tokenized_text']\n",
        "corpus"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "0       [wife, took, birthday, breakfast, excellent, ,...\n1       [idea, people, give, bad, reviews, place, , go...\n2       [love, gyro, plate, , rice, good, also, dig, c...\n3       [rosie, , dakota, , love, chaparral, dog, park...\n4       [general, manager, scott, petello, good, egg, ...\n                              ...                        \n9995    [first, visit, , lunch, today, , used, groupon...\n9996    [called, house, deliciousness, , \\n\\n, could, ...\n9997    [recently, visited, olive, ivy, business, last...\n9998    [nephew, moved, scottsdale, recently, bunch, f...\n9999    [, , , locations, , , star, average, , think, ...\nName: tokenized_text, Length: 10000, dtype: object"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "5127cd0ee09546c6afa8b7e830e25d5c",
        "deepnote_cell_type": "text-cell-h2",
        "id": "MsPBCGZYbe5r"
      },
      "source": [
        "## GloVe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "c2eba7bd5c254f8a9e392c5cc8c8e247",
        "deepnote_cell_type": "text-cell-h3",
        "id": "tNaXrMCzbe5r"
      },
      "source": [
        "### Text representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "d8b2d7c2",
        "execution_start": 1686919794700,
        "execution_millis": 64527,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "a3fa5f7c02a340e8a992d46110b270b6",
        "deepnote_cell_type": "code",
        "id": "6SQS6VNjbe5r"
      },
      "source": [
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# first thing is to load the pre-trained GloVe word vectors\n",
        "glove_file = 'glove.6B.100d.txt'\n",
        "word_vectors = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "d7409176",
        "execution_start": 1686919859240,
        "execution_millis": 3342,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "6545726f8f6a48bc89df574496591a2e",
        "deepnote_cell_type": "code",
        "id": "ixU01QExbe5r"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X = []\n",
        "for review in corpus:\n",
        "    embeddings = [word_vectors[word] for word in review if word in word_vectors]\n",
        "    # Calculating the average embedding for each review\n",
        "    average_embedding = np.mean(embeddings, axis=0) if embeddings else np.zeros(word_vectors.vector_size)\n",
        "    X.append(average_embedding)\n",
        "# We converted the preprocessed reviews in our corpus to their GloVe embeddings\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "a2ae899249d8490b8635f7bb5b9c5c56",
        "deepnote_cell_type": "text-cell-h3",
        "id": "Qtx8ZGBxbe5s"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "0cbcfa02be2e437aaca1510844cc91f6",
        "deepnote_cell_type": "text-cell-p",
        "id": "KVY4Za30be5s"
      },
      "source": [
        "We will train the model using Logistic Regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "453af5e4",
        "execution_start": 1686919862602,
        "execution_millis": 7,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "8a3e2f0fdebc40048ba3ef4062da8447",
        "deepnote_cell_type": "code",
        "id": "_l26tTnXbe5s"
      },
      "source": [
        "labels = data['stars'].tolist()\n",
        "sentiment_labels = []\n",
        "for rating in labels:\n",
        "    if rating >= 4:\n",
        "        sentiment_labels.append('good')\n",
        "    else:\n",
        "        sentiment_labels.append('bad')\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(sentiment_labels)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "c1d7626af3da409a9dc0c1bf69b5ebb7",
        "deepnote_cell_type": "text-cell-p",
        "id": "N3nhTwQObe5s"
      },
      "source": [
        "Let's perform a grid search to help find the best combination of hyperparameters for our model, and get an improved performance and generalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "6c50af27",
        "execution_start": 1686919862657,
        "execution_millis": 4,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "1a7b81bbd1564938b47e6eeb0fbdd95a",
        "deepnote_cell_type": "code",
        "id": "rGFKsF0Tbe5t"
      },
      "source": [
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'C': [0.1, 1.0, 10.0],\n",
        "    'max_iter': [100, 200, 300]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "511cd870",
        "execution_start": 1686919862659,
        "execution_millis": 2,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "75cd62f8d53549d799cc3725a0ae4aa9",
        "deepnote_cell_type": "code",
        "id": "4ME85YU_be5t"
      },
      "source": [
        "logistic_regression = LogisticRegression()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "2e30f5b8",
        "execution_start": 1686919862659,
        "execution_millis": 51931,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "0f787470ee7c4b0e97f6a52ed6806136",
        "deepnote_cell_type": "code",
        "id": "zspVrBKpbe5t",
        "outputId": "b959b91e-9413-4b0b-efed-68156c8e9c9e"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "grid_search = GridSearchCV(logistic_regression, param_grid)\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n45 fits failed out of a total of 90.\nThe score on these train-test partitions for these parameters will be set to nan.\nIf these failures are not expected, you can try to debug them by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n45 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1091, in fit\n    solver = _check_solver(self.solver, self.penalty, self.dual)\n  File \"/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 61, in _check_solver\n    raise ValueError(\nValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n\n  warnings.warn(some_fits_failed_message, FitFailedWarning)\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [     nan 0.76275       nan 0.76275       nan 0.76275       nan 0.78375\n      nan 0.78375       nan 0.78375       nan 0.7825        nan 0.782625\n      nan 0.782625]\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "execute_result",
          "execution_count": 31,
          "data": {
            "text/plain": "GridSearchCV(estimator=LogisticRegression(),\n             param_grid={'C': [0.1, 1.0, 10.0], 'max_iter': [100, 200, 300],\n                         'penalty': ['l1', 'l2']})",
            "text/html": "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=LogisticRegression(),\n             param_grid={&#x27;C&#x27;: [0.1, 1.0, 10.0], &#x27;max_iter&#x27;: [100, 200, 300],\n                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=LogisticRegression(),\n             param_grid={&#x27;C&#x27;: [0.1, 1.0, 10.0], &#x27;max_iter&#x27;: [100, 200, 300],\n                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "f278d779",
        "execution_start": 1686919914618,
        "execution_millis": 31,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "8fa045d0ed474d1cab81f91dab3cca90",
        "deepnote_cell_type": "code",
        "id": "timIlR8Vbe5u"
      },
      "source": [
        "best_model = grid_search.best_estimator_\n",
        "best_params = grid_search.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "3913b5bc",
        "execution_start": 1686919914661,
        "execution_millis": 158,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "cfc38abb8acd4bd4b18b0bb5285b01c5",
        "deepnote_cell_type": "code",
        "id": "MmEhHS8nbe5u",
        "outputId": "4f15cd0a-c3d3-4eed-83df-ce3ace62645b"
      },
      "source": [
        "predictions_glove = best_model.predict(X_test)\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Classification Report (GLOVE):\")\n",
        "print(classification_report(y_test, predictions_glove))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Best Hyperparameters: {'C': 1.0, 'max_iter': 100, 'penalty': 'l2'}\nClassification Report (GLOVE):\n              precision    recall  f1-score   support\n\n         bad       0.69      0.51      0.59       597\n        good       0.81      0.90      0.86      1403\n\n    accuracy                           0.79      2000\n   macro avg       0.75      0.71      0.72      2000\nweighted avg       0.78      0.79      0.78      2000\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "fbdf3c3cb3de48f591cc6ba956001065",
        "deepnote_cell_type": "text-cell-h2",
        "id": "PG1J6Bqtbe5u"
      },
      "source": [
        "## Tf-idf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "10a64ad083134883ae23a3ed7e29c85d",
        "deepnote_cell_type": "text-cell-h3",
        "id": "mgbBUvGtbe5u"
      },
      "source": [
        "### Text representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "b6fa2517",
        "execution_start": 1686919914865,
        "execution_millis": 5797,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "6d08d1bc28f64951ba4556a43a5d2b7f",
        "deepnote_cell_type": "code",
        "id": "Zux1Ozaabe5v"
      },
      "source": [
        "corpus = [' '.join(tokens) for tokens in data['tokenized_text']]\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=10000,\n",
        "    ngram_range=(1, 3),\n",
        "    stop_words='english'\n",
        ")\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "8d3bd249",
        "execution_start": 1686919920667,
        "execution_millis": 24,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "1620d873bf20445692843e5bec91ed1e",
        "deepnote_cell_type": "code",
        "id": "RnC3mZ3Jbe5v",
        "outputId": "468209e8-663a-47f6-cd5b-4d424a78af8f"
      },
      "source": [
        "print(\"TF-IDF representation:\")\n",
        "print(tfidf_matrix)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "TF-IDF representation:\n  (0, 1082)\t0.10430864505924704\n  (0, 5901)\t0.09852408802239128\n  (0, 1060)\t0.06784855953946042\n  (0, 7753)\t0.15028839258611135\n  (0, 8679)\t0.1603994316056082\n  (0, 6855)\t0.13338725985168615\n  (0, 8044)\t0.158613000264206\n  (0, 784)\t0.14273863191166072\n  (0, 6850)\t0.1491783379936957\n  (0, 4781)\t0.11151041420382841\n  (0, 4952)\t0.11635221552711963\n  (0, 7655)\t0.14616842078383438\n  (0, 3000)\t0.14616842078383438\n  (0, 2631)\t0.14354387271617666\n  (0, 8067)\t0.14712352363139186\n  (0, 9556)\t0.06954169339804077\n  (0, 5734)\t0.035580813949284334\n  (0, 1059)\t0.06720058437798487\n  (0, 9121)\t0.10953792155862976\n  (0, 1662)\t0.11508185107262903\n  (0, 5241)\t0.07146056360170919\n  (0, 11)\t0.09259478358382288\n  (0, 884)\t0.07956558620547736\n  (0, 6443)\t0.10290025242690515\n  (0, 1114)\t0.0671607569964568\n  :\t:\n  (9999, 5397)\t0.1331012336142193\n  (9999, 413)\t0.13372430679470323\n  (9999, 1437)\t0.13035751726508896\n  (9999, 6035)\t0.13063944685200377\n  (9999, 2764)\t0.12019410721797091\n  (9999, 2804)\t0.10324890715690418\n  (9999, 8966)\t0.17810728096113634\n  (9999, 1433)\t0.1252251181577824\n  (9999, 286)\t0.13299866941284816\n  (9999, 8305)\t0.12238702497197289\n  (9999, 4683)\t0.11659632873306362\n  (9999, 1760)\t0.32889536510337014\n  (9999, 8714)\t0.10845099968902855\n  (9999, 5103)\t0.08887254146482328\n  (9999, 7127)\t0.07109363579898118\n  (9999, 7859)\t0.06833261988764948\n  (9999, 9326)\t0.1697076297809061\n  (9999, 3697)\t0.05930330335001823\n  (9999, 7659)\t0.10026702810455204\n  (9999, 6475)\t0.21878615746156754\n  (9999, 6692)\t0.13054516733461205\n  (9999, 9556)\t0.09883541731302098\n  (9999, 3190)\t0.09663445507855001\n  (9999, 4319)\t0.1367055215370405\n  (9999, 7824)\t0.19179250237655904\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "241f92e696744387881b780ac3526b3c",
        "deepnote_cell_type": "text-cell-h3",
        "id": "2DcFOOpSbe5v"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "1508c14a",
        "execution_start": 1686919920692,
        "execution_millis": 1551,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "00b0220f95ef435a9f6aabb39e2b8609",
        "deepnote_cell_type": "code",
        "id": "ianU1YJObe5v"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "labels = data['stars'].tolist()\n",
        "sentiment_labels = []\n",
        "for rating in labels:\n",
        "    if rating >= 4:\n",
        "        sentiment_labels.append('good')\n",
        "    else:\n",
        "        sentiment_labels.append('bad')\n",
        "\n",
        "X_tfidf = tfidf_matrix.toarray()\n",
        "y_tfidf = np.array(sentiment_labels)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_tfidf, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "8f46d46",
        "execution_start": 1686919922256,
        "execution_millis": 9,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "2f3a467e7a4f4da78e92c3a84d1d0993",
        "deepnote_cell_type": "code",
        "id": "lTqKURzqbe5v"
      },
      "source": [
        "model = LogisticRegression(C=1.0, penalty='l2', solver='liblinear', max_iter=100, multi_class='auto')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "255c4e7d",
        "execution_start": 1686919922287,
        "execution_millis": 1200,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "a19d4dafd2b54f2598ded50020a44320",
        "deepnote_cell_type": "code",
        "id": "1YlLvTNjbe5w",
        "outputId": "427b4ba8-50c8-4890-d31f-cb2374b025b2"
      },
      "source": [
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "              precision    recall  f1-score   support\n\n         bad       0.83      0.54      0.65       597\n        good       0.83      0.95      0.89      1403\n\n    accuracy                           0.83      2000\n   macro avg       0.83      0.75      0.77      2000\nweighted avg       0.83      0.83      0.82      2000\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "f9ad0022a61642e18f5a601b5a24cd31",
        "deepnote_cell_type": "text-cell-h2",
        "id": "ADcXKAQKbe5w"
      },
      "source": [
        "## Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "ace8dbe720f54b2e83e757a3c1fbcf9e",
        "deepnote_cell_type": "text-cell-h3",
        "id": "wN-w05UGbe5w"
      },
      "source": [
        "### Text representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "5692ee1c",
        "execution_start": 1686919923527,
        "execution_millis": 14327,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "8ec3bdc2984944259a81448c9700586d",
        "deepnote_cell_type": "code",
        "id": "sxE1uBKlbe5w"
      },
      "source": [
        "sentences = data['tokenized_text'].tolist()\n",
        "word2vec_model = Word2Vec(\n",
        "    sentences,\n",
        "    vector_size=300,\n",
        "    window=5,\n",
        "    min_count=8,  # higher minimum count to filter out rare words\n",
        "    workers=4\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "d5d2b7da41b041a583c9d144cfb0adf4",
        "deepnote_cell_type": "text-cell-h3",
        "id": "mM7vHZRsbe5w"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "24dd320d",
        "execution_start": 1686919937847,
        "execution_millis": 16,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "2d477aa713434c7cb1e925ea822b381c",
        "deepnote_cell_type": "code",
        "id": "ZEF1tz0tbe5w"
      },
      "source": [
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "7ecfe8fe",
        "execution_start": 1686919937863,
        "execution_millis": 4276,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "b495ce392fa44ca78c15632f61d89194",
        "deepnote_cell_type": "code",
        "id": "Zjqn-atPbe5x"
      },
      "source": [
        "X_w2v = np.array([np.mean([word2vec_model.wv[token] for token in tokens if token in word2vec_model.wv], axis=0) for tokens in data['tokenized_text']])\n",
        "labels = data['stars'].tolist()\n",
        "\n",
        "sentiment_labels = []\n",
        "for rating in labels:\n",
        "    if rating >= 4:\n",
        "        sentiment_labels.append('good')\n",
        "    else:\n",
        "        sentiment_labels.append('bad')\n",
        "\n",
        "y = np.array(sentiment_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "eda6345",
        "execution_start": 1686919942141,
        "execution_millis": 23,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "9b01ac81c77544c9870b0e8df2f5f812",
        "deepnote_cell_type": "code",
        "id": "y9lXc6Azbe5x"
      },
      "source": [
        "model_w2v = LogisticRegression()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "cd785421",
        "execution_start": 1686919942147,
        "execution_millis": 23759,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "37255f1a28134fef91d3d0e3d9b4e89f",
        "deepnote_cell_type": "code",
        "id": "Y1ALa3eNbe5x",
        "outputId": "dccda41a-f465-4d82-9c1d-839baa19b3f7"
      },
      "source": [
        "grid_search = GridSearchCV(model_w2v, param_grid, cv=5)\n",
        "grid_search.fit(X_w2v, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n15 fits failed out of a total of 30.\nThe score on these train-test partitions for these parameters will be set to nan.\nIf these failures are not expected, you can try to debug them by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n15 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1091, in fit\n    solver = _check_solver(self.solver, self.penalty, self.dual)\n  File \"/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 61, in _check_solver\n    raise ValueError(\nValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n\n  warnings.warn(some_fits_failed_message, FitFailedWarning)\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [   nan 0.7643    nan 0.776     nan 0.7832]\n  warnings.warn(\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n",
          "output_type": "stream"
        },
        {
          "output_type": "execute_result",
          "execution_count": 43,
          "data": {
            "text/plain": "GridSearchCV(cv=5, estimator=LogisticRegression(),\n             param_grid={'C': [0.1, 1, 10], 'penalty': ['l1', 'l2']})",
            "text/html": "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(),\n             param_grid={&#x27;C&#x27;: [0.1, 1, 10], &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(),\n             param_grid={&#x27;C&#x27;: [0.1, 1, 10], &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "756dd9d0",
        "execution_start": 1686919965913,
        "execution_millis": 5,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "37a09f8b28c249e9baa02c4ac5068f6d",
        "deepnote_cell_type": "code",
        "id": "1NT2wTqtbe5x"
      },
      "source": [
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "50df7389",
        "execution_start": 1686919965920,
        "execution_millis": 10,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "faf450684195429f9faf044a3ff14095",
        "deepnote_cell_type": "code",
        "id": "ABjxe2Dsbe5y"
      },
      "source": [
        "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(X_w2v, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "20f593ca",
        "execution_start": 1686919965929,
        "execution_millis": 1711,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "e2c8ff12190441069ceb1d0bb9269902",
        "deepnote_cell_type": "code",
        "id": "Pr1j3alSbe5y",
        "outputId": "0bea0a79-31f4-4320-aba4-46e0e8ee7fe7"
      },
      "source": [
        "# Fitting the best model on the training data\n",
        "best_model.fit(X_train_w2v, y_train_w2v)\n",
        "\n",
        "# Evaluating the best model on the test set\n",
        "predictions_w2v = best_model.predict(X_test_w2v)\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Classification Report (Word2Vec):\")\n",
        "print(classification_report(y_test_w2v, predictions_w2v))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Best Hyperparameters: {'C': 10, 'penalty': 'l2'}\nClassification Report (Word2Vec):\n              precision    recall  f1-score   support\n\n         bad       0.68      0.50      0.57       597\n        good       0.81      0.90      0.85      1403\n\n    accuracy                           0.78      2000\n   macro avg       0.74      0.70      0.71      2000\nweighted avg       0.77      0.78      0.77      2000\n\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=7159b134-6dce-4d72-befb-c34071b2c35b' target=\"_blank\">\n",
        "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
        "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
      ],
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown",
        "id": "XQpaQDOFbe5y"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote": {},
    "orig_nbformat": 2,
    "deepnote_notebook_id": "7dca3101b0aa41c9a5b2a978b67422d3",
    "deepnote_execution_queue": [],
    "colab": {
      "provenance": []
    }
  }
}